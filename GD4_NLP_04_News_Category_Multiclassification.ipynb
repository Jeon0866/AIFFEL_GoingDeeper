{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "innovative-dragon",
   "metadata": {},
   "source": [
    "# 뉴스 카테고리 다중분류"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lucky-means",
   "metadata": {},
   "source": [
    "|평가문항|상세기준|\n",
    "|---|---|\n",
    "|1. 분류 모델의 accuracy가 기준 이상 높게 나왔는가?|3가지 단어 개수에 대해 8가지 머신러닝 기법을 적용하여 그중 최적의 솔루션을 도출하였다.|\n",
    "|2. 분류 모델의 F1 score가 기준 이상 높게 나왔는가?|Vocabulary size에 따른 각 머신러닝 모델의 성능변화 추이를 살피고, 해당 머신러닝 알고리즘의 특성에 근거해 원인을 분석하였다.|\n",
    "|3. 딥러닝 모델을 활용해 성능이 비교 및 확인되었는가?|동일한 데이터셋과 전처리 조건으로 딥러닝 모델의 성능과 비교하여 결과에 따른 원인을 분석하였다.|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incorporate-emphasis",
   "metadata": {},
   "source": [
    "# 패키지 및 라이브러리 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "exclusive-trunk",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.datasets import reuters\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer#DTM패키지\n",
    "from sklearn.feature_extraction.text import TfidfTransformer#TF-IDF패키지\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB #다항분포 나이브 베이즈 모델\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score #정확도 계산\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "revised-dealer",
   "metadata": {},
   "source": [
    "# 목표"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "previous-darkness",
   "metadata": {},
   "source": [
    "# 1. 모든 단어 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "raised-channels",
   "metadata": {},
   "source": [
    "# 2. 빈도수 상위 5,000개의 단어만 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "sharing-documentation",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/datasets/reuters.py:148: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/datasets/reuters.py:149: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = reuters.load_data(num_words=5000, test_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minus-civilian",
   "metadata": {},
   "source": [
    "## 2.1 데이터 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "peripheral-enhancement",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 2, 8, 43, 10, 447, 5, 25, 207, 270, 5, 3095, 111, 16, 369, 186, 90, 67, 7, 89, 5, 19, 102, 6, 19, 124, 15, 90, 67, 84, 22, 482, 26, 7, 48, 4, 49, 8, 864, 39, 209, 154, 6, 151, 6, 83, 11, 15, 22, 155, 11, 15, 7, 48, 9, 4579, 1005, 504, 6, 258, 6, 272, 11, 15, 22, 134, 44, 11, 15, 16, 8, 197, 1245, 90, 67, 52, 29, 209, 30, 32, 132, 6, 109, 15, 17, 12]\n"
     ]
    }
   ],
   "source": [
    "print(x_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "plastic-fitness",
   "metadata": {},
   "source": [
    "## 2.2 클래스 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "wanted-remark",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3  4  3 ... 25  3 25]\n",
      "8982\n",
      "0 ~ 45\n"
     ]
    }
   ],
   "source": [
    "print(y_train)                          # ndarray type\n",
    "print(len(y_train))                     # 개수\n",
    "print(min(y_train),'~', max(y_train))   # class 개수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "recent-folks",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46\n"
     ]
    }
   ],
   "source": [
    "num_classes = max(y_train) + 1\n",
    "print(num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intermediate-timothy",
   "metadata": {},
   "source": [
    "## 2.3 데이터 분포 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "domestic-balloon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련용 뉴스의 최대 길이 :2376\n",
      "훈련용 뉴스의 평균 길이 :145.5398574927633\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEGCAYAAACUzrmNAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Il7ecAAAACXBIWXMAAAsTAAALEwEAmpwYAAAZuUlEQVR4nO3df7RldXnf8ffHEdBGGoZAWMgPB3WSqI0SvCpZoSlqBcS0aGsU24QRiUQLEVu1GaIVNGUFmqipJiEOgThaI2VFDVOh4kggxvqDGXAEBkIYBcpMEEZRfmhEgad/7O+tx8u9s8/cmXPvufe+X2vtdfZ59o/z7MO587D3/u7vN1WFJEk78rj5TkCSNP4sFpKkXhYLSVIvi4UkqZfFQpLU6/HzncAo7LfffrVixYr5TkOSFpRrr732m1W1/3TLFmWxWLFiBRs3bpzvNCRpQUlyx0zLRnYZKskTklyT5KtJNid5V4sfluTLSbYk+Z9J9mzxvdr7LW35ioF9ndnityQ5dlQ5S5KmN8p7Fg8BL6qq5wCHA8clORI4D3hfVT0d+DZwSlv/FODbLf6+th5JngmcCDwLOA74kyTLRpi3JGmKkRWL6jzY3u7RpgJeBPxli68FXt7mT2jvactfnCQtfnFVPVRVtwFbgOePKm9J0mONtDVUkmVJNgH3AOuBrwHfqaqH2ypbgYPa/EHAnQBt+X3ATw3Gp9lm8LNOTbIxycbt27eP4GgkaekaabGoqkeq6nDgYLqzgZ8b4WetqaqJqprYf/9pb+ZLkmZpTp6zqKrvAFcBvwjsk2SyFdbBwLY2vw04BKAt/0ngW4PxabaRJM2BUbaG2j/JPm3+icBLgJvpisYr22qrgEvb/Lr2nrb8r6vrEncdcGJrLXUYsBK4ZlR5S5Iea5TPWRwIrG0tlx4HXFJVn0pyE3Bxkv8KfAW4sK1/IfCRJFuAe+laQFFVm5NcAtwEPAycVlWPjDBvSdIUWYzjWUxMTJQP5UnSzklybVVNTLdsUT7BPSorVl82bfz2c182x5lI0tyyI0FJUi+LhSSpl8VCktTLYiFJ6mWxkCT1slhIknpZLCRJvSwWkqReFgtJUi+LhSSpl8VCktTLYiFJ6mWxkCT1slhIknpZLCRJvSwWkqReFgtJUi+LhSSpl8VCktTLYiFJ6mWxkCT1slhIknpZLCRJvSwWkqReFgtJUq+RFYskhyS5KslNSTYnOaPFz06yLcmmNh0/sM2ZSbYkuSXJsQPx41psS5LVo8pZkjS9x49w3w8Db6mq65LsDVybZH1b9r6q+oPBlZM8EzgReBbwZOCzSX6mLf5j4CXAVmBDknVVddMIc5ckDRhZsaiqu4C72vwDSW4GDtrBJicAF1fVQ8BtSbYAz2/LtlTV1wGSXNzWtVhI0hyZk3sWSVYAvwB8uYVOT3J9kouSLG+xg4A7Bzbb2mIzxad+xqlJNibZuH379t19CJK0pI28WCR5EvBx4M1VdT9wPvA04HC6M4/37I7Pqao1VTVRVRP777//7tilJKkZ5T0LkuxBVyg+WlWfAKiquweWXwB8qr3dBhwysPnBLcYO4pKkOTDK1lABLgRurqr3DsQPHFjtFcCNbX4dcGKSvZIcBqwErgE2ACuTHJZkT7qb4OtGlbck6bFGeWbxS8CvAzck2dRivwO8JsnhQAG3A78JUFWbk1xCd+P6YeC0qnoEIMnpwBXAMuCiqto8wrwlSVOMsjXU54FMs+jyHWxzDnDONPHLd7SdJGm0fIJbktTLYiFJ6mWxkCT1slhIknpZLCRJvSwWkqReFgtJUi+LhSSpl8VCktRrpB0JLlQrVl823ylI0ljxzEKS1MtiIUnqZbGQJPWyWEiSelksJEm9LBaSpF4WC0lSr95ikeRXk+zd5t+R5BNJjhh9apKkcTHMmcV/qaoHkhwF/EvgQuD80aYlSRonwxSLR9rry4A1VXUZsOfoUpIkjZthisW2JB8EXg1cnmSvIbeTJC0Sw/yj/yrgCuDYqvoOsC/wtlEmJUkaL73Foqq+B9wDHNVCDwO3jjIpSdJ4GaY11FnAbwNnttAewP8YZVKSpPEyzGWoVwD/GvguQFX9A7D3KJOSJI2XYYrFD6qqgAJI8hOjTUmSNG6GKRaXtNZQ+yR5PfBZ4ILRpiVJGifD3OD+A+AvgY8DPwu8s6o+0LddkkOSXJXkpiSbk5zR4vsmWZ/k1va6vMWT5P1JtiS5fvAp8SSr2vq3Jlk124OVJM3OUMOqVtV6YP1O7vth4C1VdV3rLuTaJOuB1wJXVtW5SVYDq+luoL8UWNmmF9A9Jf6CJPsCZwETdJfCrk2yrqq+vZP5SJJmacYziyQPJLl/mumBJPf37biq7qqq69r8A8DNwEHACcDattpa4OVt/gTgw9X5Et1lrwOBY4H1VXVvKxDrgeNmd7iSpNmY8cyiqnZbi6ckK4BfAL4MHFBVd7VF3wAOaPMHAXcObLa1xWaKT/2MU4FTAQ499NDdlbokiSEvQ7X7B0fRXQb6fFV9ZdgPSPIkuvsdb66q+5P8/2VVVUlq51KeXlWtAdYATExM7JZ9SpI6wzyU9066y0U/BewHfCjJO4bZeZI96ArFR6vqEy18d7u8RHu9p8W3AYcMbH5wi80UlyTNkWGazv574HlVdVZVnQUcCfx630bpTiEuBG6uqvcOLFoHTLZoWgVcOhA/qbWKOhK4r12uugI4Jsny1nLqmBaTJM2RYS5D/QPwBOD77f1eDPd/9r9EV1RuSLKpxX4HOJfu2Y1TgDvoOioEuBw4HtgCfA84GaCq7k3yu8CGtt67q+reIT5fkrSbDFMs7gM2t2avBbwEuCbJ+wGq6k3TbVRVnwcy3TLgxdOsX8BpM+zrIuCiIXKVJI3AMMXik22adPVoUpEkjaveYlFVa/vWkSQtbsO0hvqVJF9Jcu/OPJQnSVo8hrkM9YfAvwFuaPcVJElLzDBNZ+8EbrRQSNLSNcyZxX8GLk/yN8BDk8Epz05IkhaxYYrFOcCDdM9a7DnadCRJ42iYYvHkqvpnI89EkjS2hrlncXmSY0aeiSRpbA1TLN4IfDrJP9p0VpKWpmEeyttt41pIkhamYcezWE433OkTJmNV9blRJSVJGi+9xSLJbwBn0I0jsYmui/IvAi8aaWaSpLExzD2LM4DnAXdU1Qvphkf9ziiTkiSNl2GKxfer6vsASfaqqr8Dfna0aUmSxskw9yy2JtkH+CtgfZJv0w1aJElaIoZpDfWKNnt2kquAnwQ+PdKsJEljZZguyp+WZK/Jt8AK4J+MMilJ0ngZ5p7Fx4FHkjwdWAMcAvzFSLOSJI2VYYrFo1X1MPAK4ANV9TbgwNGmJUkaJ8MUix8meQ2wCvhUi+0xupQkSeNmmGJxMvCLwDlVdVuSw4CPjDYtSdI4GaY11E3Amwbe3wacN8qkJEnjZZgzC0nSEmexkCT1mrFYJPlIez1j7tKRJI2jHZ1ZPDfJk4HXJVmeZN/BqW/HSS5Kck+SGwdiZyfZlmRTm44fWHZmki1Jbkly7ED8uBbbkmT1bA9UkjR7O7rB/afAlcBTgWvpnt6eVC2+Ix8C/gj48JT4+6rqDwYDSZ4JnAg8C3gy8NkkP9MW/zHwEmArsCHJunbTXZI0R2Y8s6iq91fVM4CLquqpVXXYwNRXKCYHR7p3yDxOAC6uqodaa6stwPPbtKWqvl5VPwAubutKkuZQ7w3uqnpjkuckOb1Nz97Fzzw9yfXtMtXyFjsIuHNgna0tNlP8MZKcmmRjko3bt2/fxRQlSYOG6UjwTcBHgZ9u00eT/NYsP+984GnA4cBdwHtmuZ/HqKo1VTVRVRP777//7tqtJInhxrP4DeAFVfVdgCTn0Q2r+oGd/bCquntyPskF/Kj7kG10HRROOrjF2EFckjRHhnnOIsAjA+8f4cdvdg8tyWAHhK8AJltKrQNOTLJX605kJXANsAFYmeSwJHvS3QRfN5vPliTN3jBnFn8OfDnJJ9v7lwMX9m2U5GPA0cB+SbYCZwFHJzmcrjXV7cBvAlTV5iSXADcBDwOnVdUjbT+nA1cAy+hutm8e8tgkSbvJMH1DvTfJ1cBRLXRyVX1liO1eM014xiJTVecA50wTvxy4vO/zJEmjM8yZBVV1HXDdiHORJI0p+4aSJPWyWEiSeu2wWCRZluSquUpGkjSedlgsWoukR5P85BzlI0kaQ8Pc4H4QuCHJeuC7k8GqetPMm0iSFpNhisUn2iRJWqKGec5ibZInAodW1S1zkJMkacwM05HgvwI2AZ9u7w9PYpcbkrSEDNN09my6cSW+A1BVm+gf+EiStIgMUyx+WFX3TYk9OopkJEnjaZgb3JuT/DtgWZKVwJuAL4w2LUnSOBnmzOK36MbGfgj4GHA/8OYR5iRJGjPDtIb6HvD2NuhRVdUDo09LkjROhmkN9bwkNwDX0z2c99Ukzx19apKkcTHMPYsLgf9QVX8LkOQougGRnj3KxCRJ42OYexaPTBYKgKr6PN1odpKkJWLGM4skR7TZv0nyQbqb2wW8Grh69KlJksbFji5DvWfK+7MG5msEuUiSxtSMxaKqXjiXiUiSxlfvDe4k+wAnASsG17eLcklaOoZpDXU58CXgBuzmQ5KWpGGKxROq6j+NPBNJ0tgaplh8JMnrgU/RdfkBQFXdO7KsFpgVqy+bNn77uS+b40wkaTSGKRY/AH4feDs/agVV2E25JC0ZwxSLtwBPr6pvjjoZSdJ4GuYJ7i3A90adiCRpfA1TLL4LbErywSTvn5z6NkpyUZJ7ktw4ENs3yfokt7bX5S2ett8tSa4feHqcJKva+rcmWTWbg5Qk7ZphisVfAefQDXh07cDU50PAcVNiq4Erq2olcGV7D/BSYGWbTgXOh6640D05/gK6oV3PmiwwkqS5M8x4Fmtns+Oq+lySFVPCJwBHt/m1dH1M/XaLf7iqCvhSkn2SHNjWXT/Z8irJeroC9LHZ5CRJmp1hnuC+jWn6gqqq2bSGOqCq7mrz3wAOaPMHAXcOrLe1xWaKT5fnqXRnJRx66KGzSE2SNJNhWkNNDMw/AfhVYN9d/eCqqiS7rUPCqloDrAGYmJiwo0NJ2o1671lU1bcGpm1V9YfAbJ82u7tdXqK93tPi24BDBtY7uMVmikuS5tAww6oeMTBNJHkDw52RTGcdMNmiaRVw6UD8pNYq6kjgvna56grgmCTL243tY1pMkjSHhvlHf3Bci4eB24FX9W2U5GN0N6j3S7KVrlXTucAlSU4B7hjYz+XA8fzomY6ToetSJMnvAhvaeu+2mxFJmnvDtIaa1bgWVfWaGRa9eJp1Czhthv1cBFw0mxwkSbvHMK2h9gL+LY8dz+Ldo0tLkjROhrkMdSlwH92DeA/1rCtJWoSGKRYHV9XUJ7ElSUvIMN19fCHJz488E0nS2BrmzOIo4LXtSe6HgNDdk372SDOTJI2NYYrFS0eehSRprA3TdPaOuUhkMXK4VUmLxTD3LCRJS5zFQpLUy2IhSeplsZAk9bJYSJJ6WSwkSb0sFpKkXhYLSVIvi4UkqZfFQpLUy2IhSeplsZAk9bJYSJJ6WSwkSb0sFpKkXhYLSVIvi4UkqZfFQpLUy2IhSeo1L8Uiye1JbkiyKcnGFts3yfokt7bX5S2eJO9PsiXJ9UmOmI+cJWkpm88zixdW1eFVNdHerwaurKqVwJXtPcBLgZVtOhU4f84zlaQlbpwuQ50ArG3za4GXD8Q/XJ0vAfskOXAe8pOkJWu+ikUBn0lybZJTW+yAqrqrzX8DOKDNHwTcObDt1hb7MUlOTbIxycbt27ePKm9JWpIeP0+fe1RVbUvy08D6JH83uLCqKkntzA6rag2wBmBiYmKntp1rK1ZfNm389nNfNseZSNJw5uXMoqq2tdd7gE8Czwfunry81F7vaatvAw4Z2PzgFpMkzZE5LxZJfiLJ3pPzwDHAjcA6YFVbbRVwaZtfB5zUWkUdCdw3cLlKkjQH5uMy1AHAJ5NMfv5fVNWnk2wALklyCnAH8Kq2/uXA8cAW4HvAyXOfsiQtbXNeLKrq68Bzpol/C3jxNPECTpuD1CRJMxinprOSpDFlsZAk9bJYSJJ6WSwkSb0sFpKkXhYLSVKv+eruQ9OwGxBJ48ozC0lSL4uFJKmXxUKS1MtiIUnqZbGQJPWyNdQCYCspSfPNMwtJUi+LhSSpl8VCktTLYiFJ6mWxkCT1sjXUAmYrKUlzxTMLSVIvi4UkqZeXoZaQmS5bgZeuJO2YxWIR2lFRkKTZ8DKUJKmXZxYCbFklaccsFpoVi4u0tFgstEO76/6HxUVa2BZMsUhyHPDfgWXAn1XVufOckqbhzXVpcVoQxSLJMuCPgZcAW4ENSdZV1U3zm5l21c4WF89EpPmxIIoF8HxgS1V9HSDJxcAJgMViibG4SPNjoRSLg4A7B95vBV4wuEKSU4FT29sHk9wyi8/ZD/jmrDJcHBbd8ee8nd5k0X0HO2mpHz8s7e/gKTMtWCjFoldVrQHW7Mo+kmysqondlNKCs9SPH/wOlvrxg9/BTBbKQ3nbgEMG3h/cYpKkObBQisUGYGWSw5LsCZwIrJvnnCRpyVgQl6Gq6uEkpwNX0DWdvaiqNo/go3bpMtYisNSPH/wOlvrxg9/BtFJV852DJGnMLZTLUJKkeWSxkCT1sljQdSWS5JYkW5Ksnu98RinJ7UluSLIpycYW2zfJ+iS3ttflLZ4k72/fy/VJjpjf7HdekouS3JPkxoHYTh9vklVt/VuTrJqPY5mtGb6Ds5Nsa7+DTUmOH1h2ZvsObkly7EB8Qf6dJDkkyVVJbkqyOckZLb6kfge7rKqW9ER3w/xrwFOBPYGvAs+c77xGeLy3A/tNif03YHWbXw2c1+aPB/43EOBI4Mvznf8sjveXgSOAG2d7vMC+wNfb6/I2v3y+j20Xv4OzgbdOs+4z29/AXsBh7W9j2UL+OwEOBI5o83sDf9+Oc0n9DnZ18sxioCuRqvoBMNmVyFJyArC2za8FXj4Q/3B1vgTsk+TAechv1qrqc8C9U8I7e7zHAuur6t6q+jawHjhu5MnvJjN8BzM5Abi4qh6qqtuALXR/Iwv276Sq7qqq69r8A8DNdL1CLKnfwa6yWEzflchB85TLXCjgM0mubV2kABxQVXe1+W8AB7T5xfrd7OzxLtbv4fR2meWiyUswLPLvIMkK4BeAL+PvYKdYLJaeo6rqCOClwGlJfnlwYXXn20umPfVSO94B5wNPAw4H7gLeM6/ZzIEkTwI+Dry5qu4fXLaEfwdDs1gssa5Eqmpbe70H+CTd5YW7Jy8vtdd72uqL9bvZ2eNddN9DVd1dVY9U1aPABXS/A1ik30GSPegKxUer6hMtvOR/BzvDYrGEuhJJ8hNJ9p6cB44BbqQ73smWHauAS9v8OuCk1jrkSOC+gdP2hWxnj/cK4Jgky9vlmmNabMGacu/pFXS/A+i+gxOT7JXkMGAlcA0L+O8kSYALgZur6r0Di5b872CnzPcd9nGY6Fo//D1da4+3z3c+IzzOp9K1YvkqsHnyWIGfAq4EbgU+C+zb4qEbdOprwA3AxHwfwyyO+WN0l1l+SHeN+ZTZHC/wOrqbvVuAk+f7uHbDd/CRdozX0/3jeODA+m9v38EtwEsH4gvy7wQ4iu4S0/XApjYdv9R+B7s62d2HJKmXl6EkSb0sFpKkXhYLSVIvi4UkqZfFQpLUy2KhBS/JgyPY5+FTemI9O8lbd2F/v5rk5iRX7Z4MZ53H7Un2m88ctDBZLKTpHU7XFn93OQV4fVW9cDfuU5ozFgstKknelmRD6yDvXS22ov1f/QVtPIPPJHliW/a8tu6mJL+f5Mb2hPK7gVe3+Kvb7p+Z5OokX0/yphk+/zXpxgu5Mcl5LfZOugfDLkzy+1PWPzDJ59rn3Jjkn7f4+Uk2tnzfNbD+7Ul+r62/MckRSa5I8rUkb2jrHN32eVm68Sf+NMlj/taT/FqSa9q+PphkWZs+1HK5Icl/3MX/JFos5vupQCenXZ2AB9vrMcAauidwHwd8im4shxXAw8Dhbb1LgF9r8zcCv9jmz6WN+QC8Fvijgc84G/gC3TgP+wHfAvaYkseTgf8L7A88Hvhr4OVt2dVM8wQ88BZ+9CT9MmDvNr/vQOxq4Nnt/e3AG9v8++ieSt67febdLX408H26J/aX0XWl/cqB7fcDngH8r8ljAP4EOAl4Ll033JP57TPf/32dxmPyzEKLyTFt+gpwHfBzdH0bAdxWVZva/LXAiiT70P3j/MUW/4ue/V9W3TgP36TrdO6AKcufB1xdVdur6mHgo3TFakc2ACcnORv4+erGWwB4VZLr2rE8i26wnkmTfTLdQDcwzwNVtR14qB0TwDXVjT3xCF13H0dN+dwX0xWGDUk2tfdPpRvQ56lJPpDkOOB+JLr/+5EWiwC/V1Uf/LFgN4bBQwOhR4AnzmL/U/exy38/VfW51k38y4APJXkv8LfAW4HnVdW3k3wIeMI0eTw6JadHB3Ka2o/P1PcB1lbVmVNzSvIcuoF+3gC8iq4/JC1xnlloMbkCeF0bt4AkByX56ZlWrqrvAA8keUELnTiw+AG6yzs74xrgXyTZL8ky4DXA3+xogyRPobt8dAHwZ3TDn/5T4LvAfUkOoBt7ZGc9v/UQ+zjg1cDnpyy/Enjl5PeTbjzqp7SWUo+rqo8D72j5SJ5ZaPGoqs8keQbwxa5Xah4Efo3uLGAmpwAXJHmU7h/2+1r8KmB1u0Tze0N+/l1JVrdtQ3fZ6tKezY4G3pbkhy3fk6rqtiRfAf6ObmS2/zPM50+xAfgj4Oktn09OyfWmJO+gGzXxcXQ90p4G/CPw5wM3xB9z5qGlyV5ntaQleVJVPdjmV9N11X3GPKe1S5IcDby1qn5lnlPRIuKZhZa6lyU5k+5v4Q66VlCSpvDMQpLUyxvckqReFgtJUi+LhSSpl8VCktTLYiFJ6vX/AHunIAk82uh/AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('훈련용 뉴스의 최대 길이 :{}'.format(max(len(l) for l in x_train)))\n",
    "print('훈련용 뉴스의 평균 길이 :{}'.format(sum(map(len, x_train))/len(x_train)))\n",
    "\n",
    "plt.hist([len(s) for s in x_train], bins=50)\n",
    "plt.xlabel('length of samples')\n",
    "plt.ylabel('number of samples')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "innocent-translator",
   "metadata": {},
   "source": [
    "### 2.3.1 클래스 분포 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "blocked-skirt",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
      "  FutureWarning\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:ylabel='count'>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axe = plt.subplots(ncols=1)\n",
    "fig.set_size_inches(12,5)\n",
    "sns.countplot(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fiscal-hormone",
   "metadata": {},
   "source": [
    "# 3. 직접 단어 개수를 설정해서 사용\n",
    "* 위 단계에서 5000으로 제시된 nun_words를 다양하게 바꾸어서 확인\n",
    "* NB, CNB, Logistic Regression, SVM, Decision Tree, RandomForest, Gradient Boost, Voting 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rational-arabic",
   "metadata": {},
   "source": [
    "## 3.1 num_words 설정 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "after-midnight",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/datasets/reuters.py:148: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/datasets/reuters.py:149: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
     ]
    }
   ],
   "source": [
    "(x_train3, y_train3), (x_test3, y_test3) = reuters.load_data(num_words=3000, test_split=0.2)\n",
    "(x_train5, y_train5), (x_test5, y_test5) = reuters.load_data(num_words=5000, test_split=0.2)\n",
    "(x_train10, y_train10), (x_test10, y_test10) = reuters.load_data(num_words=10000, test_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "federal-concept",
   "metadata": {},
   "source": [
    "## 3.2 number -> text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "guided-liberal",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "185\n"
     ]
    }
   ],
   "source": [
    "word_index = reuters.get_word_index(path=\"reuters_word_index.json\")\n",
    "print(word_index['four'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "resident-cable",
   "metadata": {},
   "source": [
    "dictionary 형태로 값이 저장되어 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "typical-pasta",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_word = {index+3 : word for word, index in word_index.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pursuant-halifax",
   "metadata": {},
   "source": [
    "### 3.2.1 OOV 문제 및  UNK token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "attempted-possession",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, token in enumerate((\"<pad>\", \"<sos>\", \"<unk>\")):\n",
    "    index_to_word[index]=token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "descending-frontier",
   "metadata": {},
   "outputs": [],
   "source": [
    "def number_to_text(dataset):\n",
    "    decoded = []\n",
    "    for i in range(len(dataset)):\n",
    "        t = ' '.join([index_to_word[index] for index in dataset[i]])\n",
    "        decoded.append(t)\n",
    "    return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "productive-psychology",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train03 = number_to_text(x_train3)\n",
    "x_test03 = number_to_text(x_test3)\n",
    "\n",
    "x_train05 = number_to_text(x_train5)\n",
    "x_test05 = number_to_text(x_test5)\n",
    "\n",
    "x_train010 = number_to_text(x_train10)\n",
    "x_test010 = number_to_text(x_test10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "continuous-course",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<sos> <unk> <unk> said as a result of its december acquisition of <unk> co it expects earnings per share in 1987 of 1 15 to 1 30 dlrs per share up from 70 cts in 1986 the company said pretax net should rise to nine to 10 mln dlrs from six mln dlrs in 1986 and <unk> operation revenues to 19 to 22 mln dlrs from 12 5 mln dlrs it said cash flow per share this year should be 2 50 to three dlrs reuter 3'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train03[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "personal-feedback",
   "metadata": {},
   "source": [
    "## 3.3 벡터화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solved-insulin",
   "metadata": {},
   "source": [
    "### 3.3.1 DTM (Document-Term Matrix)\n",
    "* BoW들을 결합한 표현 방법\n",
    "* sklearn CountVectorizer() 사용\n",
    "* 단점\n",
    " * 희소표현(Sparse representation)\n",
    " * 단순 빈도 수 기반 접근\n",
    "\n",
    "\n",
    "* [딥러닝을 이용한 자연어 처리 입문 : DTM](https://wikidocs.net/24559)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "skilled-harvard",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dtm(x_train, x_test):\n",
    "    dtmvector = CountVectorizer()\n",
    "    dtm = dtmvector.fit_transform(x_train)\n",
    "    dtm_test = dtmvector.transform(x_test)\n",
    "    return dtm, dtm_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "unnecessary-county",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8982, 2919)\n",
      "(8982, 4867)\n",
      "(8982, 9670)\n",
      "------------------------------\n",
      "(2246, 2919)\n",
      "(2246, 4867)\n",
      "(2246, 9670)\n"
     ]
    }
   ],
   "source": [
    "x_train_dtm3, x_test_dtm3 = make_dtm(x_train03, x_test03)\n",
    "x_train_dtm5, x_test_dtm5 = make_dtm(x_train05, x_test05)\n",
    "x_train_dtm10, x_test_dtm10 = make_dtm(x_train010, x_test010)\n",
    "\n",
    "print(x_train_dtm3.shape)\n",
    "print(x_train_dtm5.shape)\n",
    "print(x_train_dtm10.shape)\n",
    "print('-'*30)\n",
    "print(x_test_dtm3.shape)\n",
    "print(x_test_dtm5.shape)\n",
    "print(x_test_dtm10.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enclosed-audio",
   "metadata": {},
   "source": [
    "### 3.3.2 TF_IDF (Term Frequency-Inverse Document Frequency)\n",
    "\n",
    "* DTM 내의 각 단어들마다 중요한 정도를 가중치로 주는 방법, 사용 방법은 우선 DTM을 만든 후, TF-IDF 가중치를 부여합니다.\n",
    "* TF-IDF는 TF와 IDF를 곱한 값\n",
    "* TF는 각 문서에서의 특정 단어 의 등장 횟수.\n",
    "* DF는 특정 단어가 등장한 문서의 수.\n",
    "* IDF는 DF의 반비래수 $idf(d, t) = log(\\frac{n}{1+df(t)})$\n",
    "(문서의 수 n이 커질 수록, IDF의 값은 기하급수적으로 커지기 때문에 log 사용 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "powerful-newspaper",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_tfidf(x_train_dtm, x_test_dtm):\n",
    "    tfidf_transformer = TfidfTransformer()\n",
    "    tfidfv = tfidf_transformer.fit_transform(x_train_dtm)\n",
    "    tfidfv_test = tfidf_transformer.transform(x_test_dtm)\n",
    "    return tfidfv, tfidfv_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "outdoor-ceiling",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8982, 2919)\n",
      "(8982, 4867)\n",
      "(8982, 9670)\n",
      "------------------------------\n",
      "(2246, 2919)\n",
      "(2246, 4867)\n",
      "(2246, 9670)\n"
     ]
    }
   ],
   "source": [
    "x_train_tfidfv3, x_test_tfidfv3 = make_tfidf(x_train_dtm3, x_test_dtm3)\n",
    "x_train_tfidfv5, x_test_tfidfv5 = make_tfidf(x_train_dtm5, x_test_dtm5)\n",
    "x_train_tfidfv10, x_test_tfidfv10 = make_tfidf(x_train_dtm10, x_test_dtm10)\n",
    "\n",
    "print(x_train_tfidfv3.shape)\n",
    "print(x_train_tfidfv5.shape)\n",
    "print(x_train_tfidfv10.shape)\n",
    "print('-'*30)\n",
    "print(x_test_tfidfv3.shape)\n",
    "print(x_test_tfidfv5.shape)\n",
    "print(x_test_tfidfv10.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lasting-japan",
   "metadata": {},
   "source": [
    "## 3.4 ML Model 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "knowing-recording",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitting_model(tfidfv, y_train, tfidfv_test, y_test):\n",
    "    mod = MultinomialNB()\n",
    "    mod.fit(tfidfv, y_train)\n",
    "    print(classification_report(y_test, mod.predict(tfidfv_test)))\n",
    "    \n",
    "    cb = ComplementNB()\n",
    "    cb.fit(tfidfv, y_train)\n",
    "    print(classification_report(y_test, cb.predict(tfidfv_test)))\n",
    "    \n",
    "    lsvc = LinearSVC(C=1000, penalty='l1', max_iter=500, dual=False)\n",
    "    lsvc.fit(tfidfv, y_train)\n",
    "    print(classification_report(y_test, lsvc.predict(tfidfv_test)))\n",
    "    \n",
    "    lr = LogisticRegression(C=10000, penalty='l2')\n",
    "    lr.fit(tfidfv, y_train)\n",
    "    print(classification_report(y_test, lr.predict(tfidfv_test)))\n",
    "    \n",
    "    tree = DecisionTreeClassifier(max_depth=10, random_state=0)\n",
    "    tree.fit(tfidfv, y_train)\n",
    "    print(classification_report(y_test, tree.predict(tfidfv_test)))\n",
    "    \n",
    "    forest = RandomForestClassifier(n_estimators=5, random_state=0)\n",
    "    forest.fit(tfidfv, y_train)\n",
    "    print(classification_report(y_test, forest.predict(tfidfv_test)))\n",
    "    \n",
    "    grbt = GradientBoostingClassifier(random_state=0)\n",
    "    grbt.fit(tfidfv, y_train)\n",
    "    print(classification_report(y_test, grbt.predict(tfidfv_test)))\n",
    "    \n",
    "    voting = VotingClassifier(estimators=[\n",
    "        ('lr', LogisticRegression(C=10000, penalty='l2')),\n",
    "        ('cb', ComplementNB()),\n",
    "        ('grbt', GradientBoostingClassifier(random_state=0))], voting='soft', n_jobs=-1)\n",
    "    voting.fit(tfidfv, y_train)\n",
    "    print(classification_report(y_test, voting.predict(tfidfv_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "shared-andrew",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.25      0.40        12\n",
      "           1       0.49      0.82      0.61       105\n",
      "           2       0.00      0.00      0.00        20\n",
      "           3       0.88      0.88      0.88       813\n",
      "           4       0.64      0.95      0.76       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       0.00      0.00      0.00        14\n",
      "           7       0.00      0.00      0.00         3\n",
      "           8       0.00      0.00      0.00        38\n",
      "           9       1.00      0.48      0.65        25\n",
      "          10       1.00      0.10      0.18        30\n",
      "          11       0.45      0.76      0.56        83\n",
      "          12       0.00      0.00      0.00        13\n",
      "          13       0.70      0.19      0.30        37\n",
      "          14       0.00      0.00      0.00         2\n",
      "          15       0.00      0.00      0.00         9\n",
      "          16       0.56      0.76      0.64        99\n",
      "          17       0.00      0.00      0.00        12\n",
      "          18       0.67      0.10      0.17        20\n",
      "          19       0.50      0.81      0.62       133\n",
      "          20       0.92      0.17      0.29        70\n",
      "          21       1.00      0.22      0.36        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.00      0.00      0.00        12\n",
      "          24       0.00      0.00      0.00        19\n",
      "          25       1.00      0.10      0.18        31\n",
      "          26       0.00      0.00      0.00         8\n",
      "          27       0.00      0.00      0.00         4\n",
      "          28       0.00      0.00      0.00        10\n",
      "          29       0.00      0.00      0.00         4\n",
      "          30       0.00      0.00      0.00        12\n",
      "          31       0.00      0.00      0.00        13\n",
      "          32       0.00      0.00      0.00        10\n",
      "          33       0.00      0.00      0.00         5\n",
      "          34       0.00      0.00      0.00         7\n",
      "          35       0.00      0.00      0.00         6\n",
      "          36       0.00      0.00      0.00        11\n",
      "          37       0.00      0.00      0.00         2\n",
      "          38       0.00      0.00      0.00         3\n",
      "          39       0.00      0.00      0.00         5\n",
      "          40       0.00      0.00      0.00        10\n",
      "          41       0.00      0.00      0.00         8\n",
      "          42       0.00      0.00      0.00         3\n",
      "          43       0.00      0.00      0.00         6\n",
      "          44       0.00      0.00      0.00         5\n",
      "          45       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.69      2246\n",
      "   macro avg       0.23      0.14      0.14      2246\n",
      "weighted avg       0.65      0.69      0.63      2246\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.50      0.60        12\n",
      "           1       0.64      0.85      0.73       105\n",
      "           2       0.83      0.50      0.62        20\n",
      "           3       0.92      0.89      0.90       813\n",
      "           4       0.74      0.93      0.82       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       0.86      0.86      0.86        14\n",
      "           7       1.00      0.67      0.80         3\n",
      "           8       0.64      0.18      0.29        38\n",
      "           9       0.82      0.92      0.87        25\n",
      "          10       0.93      0.83      0.88        30\n",
      "          11       0.52      0.78      0.62        83\n",
      "          12       0.00      0.00      0.00        13\n",
      "          13       0.68      0.62      0.65        37\n",
      "          14       0.00      0.00      0.00         2\n",
      "          15       0.00      0.00      0.00         9\n",
      "          16       0.64      0.78      0.70        99\n",
      "          17       0.00      0.00      0.00        12\n",
      "          18       0.52      0.60      0.56        20\n",
      "          19       0.54      0.79      0.64       133\n",
      "          20       0.85      0.31      0.46        70\n",
      "          21       0.75      0.56      0.64        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.67      0.33      0.44        12\n",
      "          24       1.00      0.05      0.10        19\n",
      "          25       0.81      0.68      0.74        31\n",
      "          26       0.88      0.88      0.88         8\n",
      "          27       0.00      0.00      0.00         4\n",
      "          28       0.25      0.20      0.22        10\n",
      "          29       0.00      0.00      0.00         4\n",
      "          30       0.00      0.00      0.00        12\n",
      "          31       0.00      0.00      0.00        13\n",
      "          32       1.00      0.70      0.82        10\n",
      "          33       1.00      0.60      0.75         5\n",
      "          34       1.00      0.86      0.92         7\n",
      "          35       1.00      0.17      0.29         6\n",
      "          36       0.00      0.00      0.00        11\n",
      "          37       1.00      0.50      0.67         2\n",
      "          38       1.00      0.33      0.50         3\n",
      "          39       0.00      0.00      0.00         5\n",
      "          40       0.00      0.00      0.00        10\n",
      "          41       1.00      0.12      0.22         8\n",
      "          42       0.00      0.00      0.00         3\n",
      "          43       1.00      0.17      0.29         6\n",
      "          44       1.00      0.80      0.89         5\n",
      "          45       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           0.76      2246\n",
      "   macro avg       0.57      0.41      0.44      2246\n",
      "weighted avg       0.75      0.76      0.74      2246\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/svm/_base.py:975: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.58      0.61        12\n",
      "           1       0.71      0.68      0.69       105\n",
      "           2       0.75      0.75      0.75        20\n",
      "           3       0.87      0.89      0.88       813\n",
      "           4       0.79      0.81      0.80       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       0.82      1.00      0.90        14\n",
      "           7       1.00      0.33      0.50         3\n",
      "           8       0.58      0.66      0.62        38\n",
      "           9       0.74      0.80      0.77        25\n",
      "          10       0.76      0.83      0.79        30\n",
      "          11       0.59      0.69      0.63        83\n",
      "          12       0.33      0.23      0.27        13\n",
      "          13       0.56      0.54      0.55        37\n",
      "          14       0.50      0.50      0.50         2\n",
      "          15       0.71      0.56      0.63         9\n",
      "          16       0.65      0.72      0.68        99\n",
      "          17       0.83      0.42      0.56        12\n",
      "          18       0.71      0.50      0.59        20\n",
      "          19       0.61      0.63      0.62       133\n",
      "          20       0.42      0.40      0.41        70\n",
      "          21       0.56      0.74      0.63        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.45      0.42      0.43        12\n",
      "          24       0.60      0.47      0.53        19\n",
      "          25       0.76      0.61      0.68        31\n",
      "          26       1.00      0.88      0.93         8\n",
      "          27       1.00      0.25      0.40         4\n",
      "          28       0.67      0.20      0.31        10\n",
      "          29       0.20      0.50      0.29         4\n",
      "          30       0.83      0.42      0.56        12\n",
      "          31       0.70      0.54      0.61        13\n",
      "          32       0.89      0.80      0.84        10\n",
      "          33       1.00      0.80      0.89         5\n",
      "          34       0.40      0.57      0.47         7\n",
      "          35       0.67      0.33      0.44         6\n",
      "          36       0.45      0.45      0.45        11\n",
      "          37       1.00      1.00      1.00         2\n",
      "          38       1.00      0.33      0.50         3\n",
      "          39       0.20      0.20      0.20         5\n",
      "          40       0.60      0.30      0.40        10\n",
      "          41       0.80      0.50      0.62         8\n",
      "          42       1.00      0.33      0.50         3\n",
      "          43       0.62      0.83      0.71         6\n",
      "          44       1.00      0.60      0.75         5\n",
      "          45       0.50      1.00      0.67         1\n",
      "\n",
      "    accuracy                           0.75      2246\n",
      "   macro avg       0.66      0.56      0.58      2246\n",
      "weighted avg       0.75      0.75      0.75      2246\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.67      0.76        12\n",
      "           1       0.77      0.75      0.76       105\n",
      "           2       0.68      0.85      0.76        20\n",
      "           3       0.91      0.92      0.91       813\n",
      "           4       0.78      0.85      0.82       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       0.92      0.86      0.89        14\n",
      "           7       1.00      0.67      0.80         3\n",
      "           8       0.65      0.74      0.69        38\n",
      "           9       0.81      0.84      0.82        25\n",
      "          10       0.89      0.83      0.86        30\n",
      "          11       0.60      0.69      0.64        83\n",
      "          12       0.57      0.31      0.40        13\n",
      "          13       0.59      0.65      0.62        37\n",
      "          14       0.50      0.50      0.50         2\n",
      "          15       0.83      0.56      0.67         9\n",
      "          16       0.67      0.72      0.69        99\n",
      "          17       0.82      0.75      0.78        12\n",
      "          18       0.80      0.60      0.69        20\n",
      "          19       0.69      0.69      0.69       133\n",
      "          20       0.60      0.46      0.52        70\n",
      "          21       0.69      0.74      0.71        27\n",
      "          22       1.00      0.29      0.44         7\n",
      "          23       0.55      0.50      0.52        12\n",
      "          24       0.67      0.53      0.59        19\n",
      "          25       0.91      0.68      0.78        31\n",
      "          26       1.00      0.88      0.93         8\n",
      "          27       1.00      0.25      0.40         4\n",
      "          28       0.50      0.40      0.44        10\n",
      "          29       0.50      0.75      0.60         4\n",
      "          30       0.86      0.50      0.63        12\n",
      "          31       0.70      0.54      0.61        13\n",
      "          32       1.00      0.80      0.89        10\n",
      "          33       0.80      0.80      0.80         5\n",
      "          34       1.00      0.43      0.60         7\n",
      "          35       1.00      0.33      0.50         6\n",
      "          36       0.33      0.27      0.30        11\n",
      "          37       1.00      0.50      0.67         2\n",
      "          38       1.00      0.33      0.50         3\n",
      "          39       0.25      0.20      0.22         5\n",
      "          40       0.75      0.30      0.43        10\n",
      "          41       0.80      0.50      0.62         8\n",
      "          42       1.00      0.67      0.80         3\n",
      "          43       0.60      1.00      0.75         6\n",
      "          44       0.67      0.80      0.73         5\n",
      "          45       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           0.79      2246\n",
      "   macro avg       0.75      0.61      0.65      2246\n",
      "weighted avg       0.80      0.79      0.79      2246\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        12\n",
      "           1       0.72      0.39      0.51       105\n",
      "           2       0.75      0.45      0.56        20\n",
      "           3       0.94      0.84      0.89       813\n",
      "           4       0.40      0.90      0.56       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       0.90      0.64      0.75        14\n",
      "           7       0.00      0.00      0.00         3\n",
      "           8       0.00      0.00      0.00        38\n",
      "           9       0.88      0.88      0.88        25\n",
      "          10       0.87      0.87      0.87        30\n",
      "          11       0.59      0.52      0.55        83\n",
      "          12       0.00      0.00      0.00        13\n",
      "          13       0.00      0.00      0.00        37\n",
      "          14       0.00      0.00      0.00         2\n",
      "          15       0.00      0.00      0.00         9\n",
      "          16       0.60      0.84      0.70        99\n",
      "          17       0.00      0.00      0.00        12\n",
      "          18       0.00      0.00      0.00        20\n",
      "          19       0.64      0.33      0.44       133\n",
      "          20       0.50      0.03      0.05        70\n",
      "          21       0.00      0.00      0.00        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.00      0.00      0.00        12\n",
      "          24       0.33      0.05      0.09        19\n",
      "          25       0.50      0.19      0.28        31\n",
      "          26       0.00      0.00      0.00         8\n",
      "          27       0.00      0.00      0.00         4\n",
      "          28       1.00      0.10      0.18        10\n",
      "          29       0.00      0.00      0.00         4\n",
      "          30       0.00      0.00      0.00        12\n",
      "          31       0.00      0.00      0.00        13\n",
      "          32       0.00      0.00      0.00        10\n",
      "          33       1.00      1.00      1.00         5\n",
      "          34       0.00      0.00      0.00         7\n",
      "          35       0.00      0.00      0.00         6\n",
      "          36       0.00      0.00      0.00        11\n",
      "          37       0.00      0.00      0.00         2\n",
      "          38       0.00      0.00      0.00         3\n",
      "          39       0.00      0.00      0.00         5\n",
      "          40       0.00      0.00      0.00        10\n",
      "          41       0.00      0.00      0.00         8\n",
      "          42       0.00      0.00      0.00         3\n",
      "          43       0.00      0.00      0.00         6\n",
      "          44       0.00      0.00      0.00         5\n",
      "          45       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.63      2246\n",
      "   macro avg       0.23      0.17      0.18      2246\n",
      "weighted avg       0.61      0.63      0.58      2246\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.30      0.58      0.40        12\n",
      "           1       0.44      0.72      0.55       105\n",
      "           2       0.12      0.10      0.11        20\n",
      "           3       0.85      0.91      0.88       813\n",
      "           4       0.66      0.83      0.73       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       0.67      0.57      0.62        14\n",
      "           7       0.33      0.33      0.33         3\n",
      "           8       0.54      0.53      0.53        38\n",
      "           9       0.69      0.44      0.54        25\n",
      "          10       0.72      0.43      0.54        30\n",
      "          11       0.54      0.59      0.56        83\n",
      "          12       0.50      0.15      0.24        13\n",
      "          13       0.41      0.32      0.36        37\n",
      "          14       0.00      0.00      0.00         2\n",
      "          15       0.00      0.00      0.00         9\n",
      "          16       0.61      0.52      0.56        99\n",
      "          17       0.00      0.00      0.00        12\n",
      "          18       0.55      0.30      0.39        20\n",
      "          19       0.56      0.53      0.55       133\n",
      "          20       0.57      0.37      0.45        70\n",
      "          21       0.68      0.56      0.61        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.00      0.00      0.00        12\n",
      "          24       0.80      0.21      0.33        19\n",
      "          25       0.93      0.42      0.58        31\n",
      "          26       1.00      0.12      0.22         8\n",
      "          27       1.00      0.25      0.40         4\n",
      "          28       0.00      0.00      0.00        10\n",
      "          29       0.00      0.00      0.00         4\n",
      "          30       0.00      0.00      0.00        12\n",
      "          31       0.00      0.00      0.00        13\n",
      "          32       1.00      0.20      0.33        10\n",
      "          33       1.00      0.80      0.89         5\n",
      "          34       0.00      0.00      0.00         7\n",
      "          35       0.00      0.00      0.00         6\n",
      "          36       0.33      0.18      0.24        11\n",
      "          37       0.00      0.00      0.00         2\n",
      "          38       0.00      0.00      0.00         3\n",
      "          39       0.00      0.00      0.00         5\n",
      "          40       0.67      0.20      0.31        10\n",
      "          41       0.00      0.00      0.00         8\n",
      "          42       0.00      0.00      0.00         3\n",
      "          43       0.33      0.17      0.22         6\n",
      "          44       1.00      0.80      0.89         5\n",
      "          45       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           0.69      2246\n",
      "   macro avg       0.41      0.29      0.31      2246\n",
      "weighted avg       0.66      0.69      0.66      2246\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.58      0.70        12\n",
      "           1       0.76      0.70      0.73       105\n",
      "           2       0.67      0.70      0.68        20\n",
      "           3       0.89      0.92      0.91       813\n",
      "           4       0.77      0.84      0.80       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       0.93      0.93      0.93        14\n",
      "           7       0.20      0.33      0.25         3\n",
      "           8       0.62      0.66      0.64        38\n",
      "           9       0.90      0.76      0.83        25\n",
      "          10       0.86      0.80      0.83        30\n",
      "          11       0.66      0.69      0.67        83\n",
      "          12       0.21      0.23      0.22        13\n",
      "          13       0.54      0.41      0.46        37\n",
      "          14       0.50      1.00      0.67         2\n",
      "          15       0.50      0.22      0.31         9\n",
      "          16       0.71      0.71      0.71        99\n",
      "          17       0.42      0.42      0.42        12\n",
      "          18       0.61      0.55      0.58        20\n",
      "          19       0.73      0.68      0.71       133\n",
      "          20       0.68      0.51      0.59        70\n",
      "          21       0.62      0.67      0.64        27\n",
      "          22       0.25      0.14      0.18         7\n",
      "          23       0.62      0.67      0.64        12\n",
      "          24       0.65      0.58      0.61        19\n",
      "          25       0.90      0.61      0.73        31\n",
      "          26       0.75      0.75      0.75         8\n",
      "          27       0.25      0.25      0.25         4\n",
      "          28       0.38      0.30      0.33        10\n",
      "          29       0.14      0.25      0.18         4\n",
      "          30       0.43      0.50      0.46        12\n",
      "          31       0.58      0.54      0.56        13\n",
      "          32       1.00      0.70      0.82        10\n",
      "          33       0.80      0.80      0.80         5\n",
      "          34       0.40      0.29      0.33         7\n",
      "          35       1.00      0.67      0.80         6\n",
      "          36       0.60      0.55      0.57        11\n",
      "          37       0.67      1.00      0.80         2\n",
      "          38       0.50      0.33      0.40         3\n",
      "          39       0.25      0.20      0.22         5\n",
      "          40       0.67      0.40      0.50        10\n",
      "          41       0.43      0.38      0.40         8\n",
      "          42       0.67      0.67      0.67         3\n",
      "          43       0.80      0.67      0.73         6\n",
      "          44       1.00      0.80      0.89         5\n",
      "          45       0.50      1.00      0.67         1\n",
      "\n",
      "    accuracy                           0.78      2246\n",
      "   macro avg       0.61      0.57      0.58      2246\n",
      "weighted avg       0.77      0.78      0.77      2246\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.67      0.76        12\n",
      "           1       0.77      0.78      0.77       105\n",
      "           2       0.73      0.80      0.76        20\n",
      "           3       0.92      0.94      0.93       813\n",
      "           4       0.82      0.88      0.85       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       0.93      0.93      0.93        14\n",
      "           7       1.00      0.33      0.50         3\n",
      "           8       0.68      0.74      0.71        38\n",
      "           9       0.80      0.80      0.80        25\n",
      "          10       0.90      0.87      0.88        30\n",
      "          11       0.65      0.70      0.67        83\n",
      "          12       0.50      0.38      0.43        13\n",
      "          13       0.62      0.62      0.62        37\n",
      "          14       0.67      1.00      0.80         2\n",
      "          15       0.67      0.44      0.53         9\n",
      "          16       0.70      0.73      0.71        99\n",
      "          17       0.62      0.67      0.64        12\n",
      "          18       0.76      0.65      0.70        20\n",
      "          19       0.71      0.71      0.71       133\n",
      "          20       0.58      0.44      0.50        70\n",
      "          21       0.66      0.78      0.71        27\n",
      "          22       0.50      0.14      0.22         7\n",
      "          23       0.64      0.75      0.69        12\n",
      "          24       0.65      0.58      0.61        19\n",
      "          25       0.92      0.71      0.80        31\n",
      "          26       0.88      0.88      0.88         8\n",
      "          27       0.50      0.25      0.33         4\n",
      "          28       0.38      0.30      0.33        10\n",
      "          29       0.50      0.75      0.60         4\n",
      "          30       0.67      0.50      0.57        12\n",
      "          31       0.64      0.69      0.67        13\n",
      "          32       1.00      0.80      0.89        10\n",
      "          33       1.00      0.80      0.89         5\n",
      "          34       0.75      0.43      0.55         7\n",
      "          35       1.00      0.67      0.80         6\n",
      "          36       0.45      0.45      0.45        11\n",
      "          37       0.67      1.00      0.80         2\n",
      "          38       0.50      0.33      0.40         3\n",
      "          39       0.20      0.20      0.20         5\n",
      "          40       0.60      0.30      0.40        10\n",
      "          41       0.75      0.38      0.50         8\n",
      "          42       0.67      0.67      0.67         3\n",
      "          43       0.71      0.83      0.77         6\n",
      "          44       1.00      0.80      0.89         5\n",
      "          45       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           0.81      2246\n",
      "   macro avg       0.70      0.63      0.65      2246\n",
      "weighted avg       0.81      0.81      0.81      2246\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "numword_3000_model = fitting_model(x_train_tfidfv3, \n",
    "                                   y_train3, \n",
    "                                   x_test_tfidfv3, \n",
    "                                   y_test3)\n",
    "print(numword_3000_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "following-sunday",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        12\n",
      "           1       0.50      0.80      0.62       105\n",
      "           2       0.00      0.00      0.00        20\n",
      "           3       0.86      0.89      0.87       813\n",
      "           4       0.59      0.95      0.73       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       0.00      0.00      0.00        14\n",
      "           7       0.00      0.00      0.00         3\n",
      "           8       0.00      0.00      0.00        38\n",
      "           9       1.00      0.28      0.44        25\n",
      "          10       0.00      0.00      0.00        30\n",
      "          11       0.48      0.73      0.58        83\n",
      "          12       0.00      0.00      0.00        13\n",
      "          13       1.00      0.14      0.24        37\n",
      "          14       0.00      0.00      0.00         2\n",
      "          15       0.00      0.00      0.00         9\n",
      "          16       0.60      0.66      0.62        99\n",
      "          17       0.00      0.00      0.00        12\n",
      "          18       0.00      0.00      0.00        20\n",
      "          19       0.51      0.81      0.63       133\n",
      "          20       0.90      0.13      0.23        70\n",
      "          21       0.00      0.00      0.00        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.00      0.00      0.00        12\n",
      "          24       0.00      0.00      0.00        19\n",
      "          25       1.00      0.06      0.12        31\n",
      "          26       0.00      0.00      0.00         8\n",
      "          27       0.00      0.00      0.00         4\n",
      "          28       0.00      0.00      0.00        10\n",
      "          29       0.00      0.00      0.00         4\n",
      "          30       0.00      0.00      0.00        12\n",
      "          31       0.00      0.00      0.00        13\n",
      "          32       0.00      0.00      0.00        10\n",
      "          33       0.00      0.00      0.00         5\n",
      "          34       0.00      0.00      0.00         7\n",
      "          35       0.00      0.00      0.00         6\n",
      "          36       0.00      0.00      0.00        11\n",
      "          37       0.00      0.00      0.00         2\n",
      "          38       0.00      0.00      0.00         3\n",
      "          39       0.00      0.00      0.00         5\n",
      "          40       0.00      0.00      0.00        10\n",
      "          41       0.00      0.00      0.00         8\n",
      "          42       0.00      0.00      0.00         3\n",
      "          43       0.00      0.00      0.00         6\n",
      "          44       0.00      0.00      0.00         5\n",
      "          45       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.67      2246\n",
      "   macro avg       0.16      0.12      0.11      2246\n",
      "weighted avg       0.60      0.67      0.60      2246\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.58      0.70        12\n",
      "           1       0.63      0.86      0.73       105\n",
      "           2       0.91      0.50      0.65        20\n",
      "           3       0.91      0.89      0.90       813\n",
      "           4       0.74      0.92      0.82       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       0.86      0.86      0.86        14\n",
      "           7       1.00      0.67      0.80         3\n",
      "           8       0.57      0.21      0.31        38\n",
      "           9       0.82      0.92      0.87        25\n",
      "          10       0.96      0.80      0.87        30\n",
      "          11       0.54      0.76      0.63        83\n",
      "          12       0.00      0.00      0.00        13\n",
      "          13       0.69      0.59      0.64        37\n",
      "          14       0.00      0.00      0.00         2\n",
      "          15       0.00      0.00      0.00         9\n",
      "          16       0.67      0.79      0.72        99\n",
      "          17       0.00      0.00      0.00        12\n",
      "          18       0.55      0.60      0.57        20\n",
      "          19       0.56      0.80      0.66       133\n",
      "          20       0.79      0.33      0.46        70\n",
      "          21       0.78      0.67      0.72        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.67      0.33      0.44        12\n",
      "          24       0.67      0.11      0.18        19\n",
      "          25       0.86      0.77      0.81        31\n",
      "          26       0.88      0.88      0.88         8\n",
      "          27       1.00      0.25      0.40         4\n",
      "          28       0.33      0.20      0.25        10\n",
      "          29       0.00      0.00      0.00         4\n",
      "          30       0.00      0.00      0.00        12\n",
      "          31       1.00      0.15      0.27        13\n",
      "          32       1.00      0.70      0.82        10\n",
      "          33       1.00      0.80      0.89         5\n",
      "          34       1.00      0.71      0.83         7\n",
      "          35       1.00      0.17      0.29         6\n",
      "          36       0.00      0.00      0.00        11\n",
      "          37       1.00      0.50      0.67         2\n",
      "          38       1.00      0.33      0.50         3\n",
      "          39       0.00      0.00      0.00         5\n",
      "          40       0.00      0.00      0.00        10\n",
      "          41       0.67      0.25      0.36         8\n",
      "          42       1.00      0.33      0.50         3\n",
      "          43       1.00      0.17      0.29         6\n",
      "          44       1.00      0.80      0.89         5\n",
      "          45       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           0.77      2246\n",
      "   macro avg       0.63      0.44      0.48      2246\n",
      "weighted avg       0.76      0.77      0.75      2246\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/svm/_base.py:975: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.67      0.76        12\n",
      "           1       0.69      0.74      0.72       105\n",
      "           2       0.78      0.70      0.74        20\n",
      "           3       0.89      0.90      0.90       813\n",
      "           4       0.81      0.83      0.82       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       0.93      0.93      0.93        14\n",
      "           7       0.50      0.33      0.40         3\n",
      "           8       0.61      0.66      0.63        38\n",
      "           9       0.79      0.88      0.83        25\n",
      "          10       0.85      0.77      0.81        30\n",
      "          11       0.63      0.72      0.67        83\n",
      "          12       0.38      0.38      0.38        13\n",
      "          13       0.50      0.51      0.51        37\n",
      "          14       0.50      0.50      0.50         2\n",
      "          15       0.80      0.44      0.57         9\n",
      "          16       0.62      0.69      0.65        99\n",
      "          17       0.50      0.25      0.33        12\n",
      "          18       0.79      0.55      0.65        20\n",
      "          19       0.60      0.62      0.61       133\n",
      "          20       0.50      0.49      0.49        70\n",
      "          21       0.55      0.78      0.65        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.56      0.42      0.48        12\n",
      "          24       0.62      0.53      0.57        19\n",
      "          25       0.68      0.55      0.61        31\n",
      "          26       1.00      0.88      0.93         8\n",
      "          27       0.50      0.25      0.33         4\n",
      "          28       0.67      0.40      0.50        10\n",
      "          29       0.50      0.75      0.60         4\n",
      "          30       0.57      0.33      0.42        12\n",
      "          31       0.67      0.62      0.64        13\n",
      "          32       1.00      0.70      0.82        10\n",
      "          33       0.67      0.80      0.73         5\n",
      "          34       0.67      0.57      0.62         7\n",
      "          35       1.00      0.33      0.50         6\n",
      "          36       0.55      0.55      0.55        11\n",
      "          37       0.50      1.00      0.67         2\n",
      "          38       0.50      0.33      0.40         3\n",
      "          39       0.33      0.20      0.25         5\n",
      "          40       0.67      0.40      0.50        10\n",
      "          41       0.44      0.50      0.47         8\n",
      "          42       0.50      0.33      0.40         3\n",
      "          43       0.56      0.83      0.67         6\n",
      "          44       1.00      0.80      0.89         5\n",
      "          45       0.50      1.00      0.67         1\n",
      "\n",
      "    accuracy                           0.77      2246\n",
      "   macro avg       0.63      0.57      0.58      2246\n",
      "weighted avg       0.76      0.77      0.76      2246\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.67      0.76        12\n",
      "           1       0.77      0.80      0.79       105\n",
      "           2       0.74      0.85      0.79        20\n",
      "           3       0.91      0.93      0.92       813\n",
      "           4       0.81      0.87      0.84       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       0.92      0.86      0.89        14\n",
      "           7       1.00      0.67      0.80         3\n",
      "           8       0.64      0.74      0.68        38\n",
      "           9       0.81      0.88      0.85        25\n",
      "          10       0.93      0.87      0.90        30\n",
      "          11       0.64      0.73      0.68        83\n",
      "          12       0.57      0.31      0.40        13\n",
      "          13       0.64      0.62      0.63        37\n",
      "          14       0.50      0.50      0.50         2\n",
      "          15       0.83      0.56      0.67         9\n",
      "          16       0.67      0.73      0.70        99\n",
      "          17       0.82      0.75      0.78        12\n",
      "          18       0.80      0.60      0.69        20\n",
      "          19       0.66      0.68      0.67       133\n",
      "          20       0.61      0.47      0.53        70\n",
      "          21       0.62      0.78      0.69        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.55      0.50      0.52        12\n",
      "          24       0.69      0.58      0.63        19\n",
      "          25       0.91      0.65      0.75        31\n",
      "          26       1.00      0.88      0.93         8\n",
      "          27       1.00      0.25      0.40         4\n",
      "          28       0.67      0.40      0.50        10\n",
      "          29       0.50      0.75      0.60         4\n",
      "          30       1.00      0.42      0.59        12\n",
      "          31       0.70      0.54      0.61        13\n",
      "          32       1.00      0.80      0.89        10\n",
      "          33       0.80      0.80      0.80         5\n",
      "          34       1.00      0.29      0.44         7\n",
      "          35       1.00      0.33      0.50         6\n",
      "          36       0.38      0.27      0.32        11\n",
      "          37       0.50      0.50      0.50         2\n",
      "          38       0.50      0.33      0.40         3\n",
      "          39       0.40      0.40      0.40         5\n",
      "          40       0.75      0.30      0.43        10\n",
      "          41       0.83      0.62      0.71         8\n",
      "          42       1.00      0.67      0.80         3\n",
      "          43       0.67      1.00      0.80         6\n",
      "          44       1.00      0.80      0.89         5\n",
      "          45       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           0.81      2246\n",
      "   macro avg       0.73      0.61      0.64      2246\n",
      "weighted avg       0.80      0.81      0.80      2246\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        12\n",
      "           1       0.72      0.40      0.52       105\n",
      "           2       0.60      0.45      0.51        20\n",
      "           3       0.94      0.84      0.89       813\n",
      "           4       0.39      0.91      0.55       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       1.00      0.57      0.73        14\n",
      "           7       0.00      0.00      0.00         3\n",
      "           8       0.00      0.00      0.00        38\n",
      "           9       0.88      0.88      0.88        25\n",
      "          10       0.87      0.87      0.87        30\n",
      "          11       0.62      0.48      0.54        83\n",
      "          12       0.17      0.08      0.11        13\n",
      "          13       0.00      0.00      0.00        37\n",
      "          14       0.00      0.00      0.00         2\n",
      "          15       0.00      0.00      0.00         9\n",
      "          16       0.60      0.82      0.69        99\n",
      "          17       0.00      0.00      0.00        12\n",
      "          18       0.00      0.00      0.00        20\n",
      "          19       0.62      0.26      0.37       133\n",
      "          20       0.33      0.03      0.05        70\n",
      "          21       0.00      0.00      0.00        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.00      0.00      0.00        12\n",
      "          24       1.00      0.05      0.10        19\n",
      "          25       0.86      0.19      0.32        31\n",
      "          26       0.00      0.00      0.00         8\n",
      "          27       0.00      0.00      0.00         4\n",
      "          28       0.50      0.10      0.17        10\n",
      "          29       0.00      0.00      0.00         4\n",
      "          30       0.00      0.00      0.00        12\n",
      "          31       0.00      0.00      0.00        13\n",
      "          32       0.00      0.00      0.00        10\n",
      "          33       0.83      1.00      0.91         5\n",
      "          34       0.00      0.00      0.00         7\n",
      "          35       0.00      0.00      0.00         6\n",
      "          36       0.00      0.00      0.00        11\n",
      "          37       0.00      0.00      0.00         2\n",
      "          38       0.00      0.00      0.00         3\n",
      "          39       0.00      0.00      0.00         5\n",
      "          40       0.00      0.00      0.00        10\n",
      "          41       0.00      0.00      0.00         8\n",
      "          42       0.00      0.00      0.00         3\n",
      "          43       0.00      0.00      0.00         6\n",
      "          44       0.00      0.00      0.00         5\n",
      "          45       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.62      2246\n",
      "   macro avg       0.24      0.17      0.18      2246\n",
      "weighted avg       0.61      0.62      0.57      2246\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.28      0.42      0.33        12\n",
      "           1       0.42      0.78      0.55       105\n",
      "           2       0.44      0.35      0.39        20\n",
      "           3       0.84      0.90      0.87       813\n",
      "           4       0.68      0.84      0.75       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       0.86      0.43      0.57        14\n",
      "           7       1.00      0.33      0.50         3\n",
      "           8       0.59      0.53      0.56        38\n",
      "           9       0.71      0.40      0.51        25\n",
      "          10       0.89      0.53      0.67        30\n",
      "          11       0.57      0.69      0.62        83\n",
      "          12       0.33      0.15      0.21        13\n",
      "          13       0.46      0.32      0.38        37\n",
      "          14       0.00      0.00      0.00         2\n",
      "          15       1.00      0.11      0.20         9\n",
      "          16       0.70      0.67      0.68        99\n",
      "          17       0.00      0.00      0.00        12\n",
      "          18       0.60      0.45      0.51        20\n",
      "          19       0.62      0.64      0.63       133\n",
      "          20       0.46      0.33      0.38        70\n",
      "          21       0.65      0.41      0.50        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.75      0.25      0.38        12\n",
      "          24       0.33      0.05      0.09        19\n",
      "          25       0.87      0.42      0.57        31\n",
      "          26       1.00      0.12      0.22         8\n",
      "          27       1.00      0.25      0.40         4\n",
      "          28       0.00      0.00      0.00        10\n",
      "          29       0.33      0.25      0.29         4\n",
      "          30       0.00      0.00      0.00        12\n",
      "          31       0.00      0.00      0.00        13\n",
      "          32       1.00      0.30      0.46        10\n",
      "          33       1.00      0.20      0.33         5\n",
      "          34       0.00      0.00      0.00         7\n",
      "          35       1.00      0.17      0.29         6\n",
      "          36       0.33      0.09      0.14        11\n",
      "          37       1.00      0.50      0.67         2\n",
      "          38       0.00      0.00      0.00         3\n",
      "          39       0.00      0.00      0.00         5\n",
      "          40       1.00      0.20      0.33        10\n",
      "          41       0.25      0.12      0.17         8\n",
      "          42       0.00      0.00      0.00         3\n",
      "          43       1.00      0.33      0.50         6\n",
      "          44       1.00      0.80      0.89         5\n",
      "          45       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           0.70      2246\n",
      "   macro avg       0.54      0.31      0.36      2246\n",
      "weighted avg       0.69      0.70      0.68      2246\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.67      0.76        12\n",
      "           1       0.80      0.68      0.73       105\n",
      "           2       0.70      0.70      0.70        20\n",
      "           3       0.90      0.90      0.90       813\n",
      "           4       0.76      0.83      0.79       474\n",
      "           5       0.14      0.20      0.17         5\n",
      "           6       0.93      0.93      0.93        14\n",
      "           7       0.50      0.33      0.40         3\n",
      "           8       0.64      0.66      0.65        38\n",
      "           9       0.91      0.84      0.87        25\n",
      "          10       0.87      0.87      0.87        30\n",
      "          11       0.62      0.66      0.64        83\n",
      "          12       0.46      0.46      0.46        13\n",
      "          13       0.55      0.43      0.48        37\n",
      "          14       0.08      0.50      0.14         2\n",
      "          15       0.33      0.22      0.27         9\n",
      "          16       0.72      0.77      0.75        99\n",
      "          17       0.33      0.33      0.33        12\n",
      "          18       0.61      0.55      0.58        20\n",
      "          19       0.71      0.65      0.68       133\n",
      "          20       0.56      0.44      0.50        70\n",
      "          21       0.67      0.67      0.67        27\n",
      "          22       0.50      0.14      0.22         7\n",
      "          23       0.36      0.42      0.38        12\n",
      "          24       0.71      0.63      0.67        19\n",
      "          25       0.91      0.65      0.75        31\n",
      "          26       0.75      0.75      0.75         8\n",
      "          27       0.40      0.50      0.44         4\n",
      "          28       0.38      0.30      0.33        10\n",
      "          29       0.22      0.50      0.31         4\n",
      "          30       0.38      0.42      0.40        12\n",
      "          31       0.60      0.46      0.52        13\n",
      "          32       0.88      0.70      0.78        10\n",
      "          33       0.71      1.00      0.83         5\n",
      "          34       0.50      0.29      0.36         7\n",
      "          35       1.00      0.50      0.67         6\n",
      "          36       0.67      0.55      0.60        11\n",
      "          37       0.67      1.00      0.80         2\n",
      "          38       0.25      0.33      0.29         3\n",
      "          39       0.25      0.20      0.22         5\n",
      "          40       0.71      0.50      0.59        10\n",
      "          41       0.44      0.50      0.47         8\n",
      "          42       0.75      1.00      0.86         3\n",
      "          43       0.50      0.67      0.57         6\n",
      "          44       1.00      0.80      0.89         5\n",
      "          45       0.50      1.00      0.67         1\n",
      "\n",
      "    accuracy                           0.77      2246\n",
      "   macro avg       0.60      0.59      0.58      2246\n",
      "weighted avg       0.77      0.77      0.77      2246\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.75      0.82        12\n",
      "           1       0.80      0.77      0.79       105\n",
      "           2       0.71      0.85      0.77        20\n",
      "           3       0.92      0.94      0.93       813\n",
      "           4       0.82      0.88      0.85       474\n",
      "           5       0.33      0.20      0.25         5\n",
      "           6       0.93      0.93      0.93        14\n",
      "           7       0.67      0.67      0.67         3\n",
      "           8       0.72      0.68      0.70        38\n",
      "           9       0.81      0.84      0.82        25\n",
      "          10       0.93      0.90      0.92        30\n",
      "          11       0.67      0.70      0.68        83\n",
      "          12       0.60      0.46      0.52        13\n",
      "          13       0.68      0.62      0.65        37\n",
      "          14       0.12      0.50      0.20         2\n",
      "          15       0.67      0.44      0.53         9\n",
      "          16       0.74      0.74      0.74        99\n",
      "          17       0.57      0.67      0.62        12\n",
      "          18       0.72      0.65      0.68        20\n",
      "          19       0.73      0.68      0.71       133\n",
      "          20       0.61      0.49      0.54        70\n",
      "          21       0.66      0.78      0.71        27\n",
      "          22       0.50      0.14      0.22         7\n",
      "          23       0.57      0.67      0.62        12\n",
      "          24       0.75      0.63      0.69        19\n",
      "          25       0.96      0.74      0.84        31\n",
      "          26       0.88      0.88      0.88         8\n",
      "          27       0.67      0.50      0.57         4\n",
      "          28       0.44      0.40      0.42        10\n",
      "          29       0.50      0.75      0.60         4\n",
      "          30       0.62      0.42      0.50        12\n",
      "          31       0.75      0.69      0.72        13\n",
      "          32       1.00      0.80      0.89        10\n",
      "          33       0.71      1.00      0.83         5\n",
      "          34       1.00      0.43      0.60         7\n",
      "          35       1.00      0.50      0.67         6\n",
      "          36       0.45      0.45      0.45        11\n",
      "          37       0.67      1.00      0.80         2\n",
      "          38       0.50      0.33      0.40         3\n",
      "          39       0.25      0.20      0.22         5\n",
      "          40       0.80      0.40      0.53        10\n",
      "          41       0.67      0.50      0.57         8\n",
      "          42       0.75      1.00      0.86         3\n",
      "          43       0.71      0.83      0.77         6\n",
      "          44       1.00      0.80      0.89         5\n",
      "          45       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           0.82      2246\n",
      "   macro avg       0.71      0.66      0.66      2246\n",
      "weighted avg       0.82      0.82      0.81      2246\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "numword_5000_model = fitting_model(x_train_tfidfv5, \n",
    "                                   y_train5, \n",
    "                                   x_test_tfidfv5, \n",
    "                                   y_test5)\n",
    "print(numword_5000_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "prompt-implementation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        12\n",
      "           1       0.62      0.69      0.65       105\n",
      "           2       0.00      0.00      0.00        20\n",
      "           3       0.81      0.90      0.85       813\n",
      "           4       0.51      0.96      0.67       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       0.00      0.00      0.00        14\n",
      "           7       0.00      0.00      0.00         3\n",
      "           8       0.00      0.00      0.00        38\n",
      "           9       1.00      0.08      0.15        25\n",
      "          10       0.00      0.00      0.00        30\n",
      "          11       0.66      0.63      0.64        83\n",
      "          12       0.00      0.00      0.00        13\n",
      "          13       1.00      0.03      0.05        37\n",
      "          14       0.00      0.00      0.00         2\n",
      "          15       0.00      0.00      0.00         9\n",
      "          16       0.69      0.56      0.61        99\n",
      "          17       0.00      0.00      0.00        12\n",
      "          18       0.00      0.00      0.00        20\n",
      "          19       0.60      0.78      0.68       133\n",
      "          20       1.00      0.04      0.08        70\n",
      "          21       0.00      0.00      0.00        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.00      0.00      0.00        12\n",
      "          24       0.00      0.00      0.00        19\n",
      "          25       1.00      0.03      0.06        31\n",
      "          26       0.00      0.00      0.00         8\n",
      "          27       0.00      0.00      0.00         4\n",
      "          28       0.00      0.00      0.00        10\n",
      "          29       0.00      0.00      0.00         4\n",
      "          30       0.00      0.00      0.00        12\n",
      "          31       0.00      0.00      0.00        13\n",
      "          32       0.00      0.00      0.00        10\n",
      "          33       0.00      0.00      0.00         5\n",
      "          34       0.00      0.00      0.00         7\n",
      "          35       0.00      0.00      0.00         6\n",
      "          36       0.00      0.00      0.00        11\n",
      "          37       0.00      0.00      0.00         2\n",
      "          38       0.00      0.00      0.00         3\n",
      "          39       0.00      0.00      0.00         5\n",
      "          40       0.00      0.00      0.00        10\n",
      "          41       0.00      0.00      0.00         8\n",
      "          42       0.00      0.00      0.00         3\n",
      "          43       0.00      0.00      0.00         6\n",
      "          44       0.00      0.00      0.00         5\n",
      "          45       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.66      2246\n",
      "   macro avg       0.17      0.10      0.10      2246\n",
      "weighted avg       0.59      0.66      0.58      2246\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.67      0.76        12\n",
      "           1       0.64      0.88      0.74       105\n",
      "           2       0.91      0.50      0.65        20\n",
      "           3       0.91      0.89      0.90       813\n",
      "           4       0.75      0.92      0.83       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       0.93      0.93      0.93        14\n",
      "           7       1.00      0.67      0.80         3\n",
      "           8       0.50      0.13      0.21        38\n",
      "           9       0.82      0.92      0.87        25\n",
      "          10       0.96      0.80      0.87        30\n",
      "          11       0.55      0.73      0.63        83\n",
      "          12       0.00      0.00      0.00        13\n",
      "          13       0.58      0.59      0.59        37\n",
      "          14       0.00      0.00      0.00         2\n",
      "          15       0.50      0.11      0.18         9\n",
      "          16       0.67      0.79      0.73        99\n",
      "          17       0.00      0.00      0.00        12\n",
      "          18       0.55      0.60      0.57        20\n",
      "          19       0.55      0.80      0.65       133\n",
      "          20       0.75      0.30      0.43        70\n",
      "          21       0.74      0.63      0.68        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.75      0.50      0.60        12\n",
      "          24       0.50      0.11      0.17        19\n",
      "          25       0.85      0.74      0.79        31\n",
      "          26       0.88      0.88      0.88         8\n",
      "          27       1.00      0.25      0.40         4\n",
      "          28       0.25      0.10      0.14        10\n",
      "          29       0.00      0.00      0.00         4\n",
      "          30       0.00      0.00      0.00        12\n",
      "          31       1.00      0.31      0.47        13\n",
      "          32       1.00      0.70      0.82        10\n",
      "          33       1.00      0.80      0.89         5\n",
      "          34       1.00      0.71      0.83         7\n",
      "          35       1.00      0.17      0.29         6\n",
      "          36       0.00      0.00      0.00        11\n",
      "          37       0.00      0.00      0.00         2\n",
      "          38       1.00      0.33      0.50         3\n",
      "          39       1.00      0.20      0.33         5\n",
      "          40       0.00      0.00      0.00        10\n",
      "          41       0.67      0.25      0.36         8\n",
      "          42       1.00      0.33      0.50         3\n",
      "          43       1.00      0.17      0.29         6\n",
      "          44       0.67      0.80      0.73         5\n",
      "          45       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           0.77      2246\n",
      "   macro avg       0.63      0.44      0.48      2246\n",
      "weighted avg       0.75      0.77      0.75      2246\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/svm/_base.py:975: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.67      0.76        12\n",
      "           1       0.71      0.73      0.72       105\n",
      "           2       0.67      0.80      0.73        20\n",
      "           3       0.90      0.92      0.91       813\n",
      "           4       0.81      0.84      0.82       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       0.72      0.93      0.81        14\n",
      "           7       1.00      0.67      0.80         3\n",
      "           8       0.64      0.66      0.65        38\n",
      "           9       0.80      0.80      0.80        25\n",
      "          10       0.89      0.83      0.86        30\n",
      "          11       0.61      0.72      0.66        83\n",
      "          12       0.33      0.31      0.32        13\n",
      "          13       0.53      0.49      0.51        37\n",
      "          14       0.20      0.50      0.29         2\n",
      "          15       0.50      0.22      0.31         9\n",
      "          16       0.65      0.72      0.68        99\n",
      "          17       1.00      0.17      0.29        12\n",
      "          18       0.79      0.55      0.65        20\n",
      "          19       0.63      0.65      0.64       133\n",
      "          20       0.52      0.46      0.49        70\n",
      "          21       0.57      0.74      0.65        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.50      0.50      0.50        12\n",
      "          24       0.67      0.53      0.59        19\n",
      "          25       0.80      0.52      0.63        31\n",
      "          26       0.88      0.88      0.88         8\n",
      "          27       1.00      0.25      0.40         4\n",
      "          28       0.44      0.40      0.42        10\n",
      "          29       0.38      0.75      0.50         4\n",
      "          30       0.75      0.50      0.60        12\n",
      "          31       0.67      0.62      0.64        13\n",
      "          32       1.00      0.70      0.82        10\n",
      "          33       0.80      0.80      0.80         5\n",
      "          34       0.44      0.57      0.50         7\n",
      "          35       1.00      0.33      0.50         6\n",
      "          36       0.70      0.64      0.67        11\n",
      "          37       1.00      0.50      0.67         2\n",
      "          38       1.00      0.33      0.50         3\n",
      "          39       0.50      0.20      0.29         5\n",
      "          40       0.43      0.30      0.35        10\n",
      "          41       0.80      0.50      0.62         8\n",
      "          42       0.67      0.67      0.67         3\n",
      "          43       0.75      1.00      0.86         6\n",
      "          44       1.00      0.80      0.89         5\n",
      "          45       0.50      1.00      0.67         1\n",
      "\n",
      "    accuracy                           0.78      2246\n",
      "   macro avg       0.67      0.58      0.59      2246\n",
      "weighted avg       0.78      0.78      0.77      2246\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.67      0.76        12\n",
      "           1       0.75      0.78      0.76       105\n",
      "           2       0.74      0.85      0.79        20\n",
      "           3       0.92      0.93      0.93       813\n",
      "           4       0.81      0.87      0.84       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       0.92      0.86      0.89        14\n",
      "           7       1.00      0.67      0.80         3\n",
      "           8       0.68      0.71      0.69        38\n",
      "           9       0.81      0.84      0.82        25\n",
      "          10       0.93      0.87      0.90        30\n",
      "          11       0.64      0.73      0.68        83\n",
      "          12       0.57      0.31      0.40        13\n",
      "          13       0.59      0.59      0.59        37\n",
      "          14       0.50      0.50      0.50         2\n",
      "          15       0.67      0.44      0.53         9\n",
      "          16       0.68      0.75      0.71        99\n",
      "          17       0.75      0.75      0.75        12\n",
      "          18       0.86      0.60      0.71        20\n",
      "          19       0.68      0.68      0.68       133\n",
      "          20       0.62      0.49      0.54        70\n",
      "          21       0.63      0.81      0.71        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.64      0.58      0.61        12\n",
      "          24       0.62      0.53      0.57        19\n",
      "          25       0.95      0.68      0.79        31\n",
      "          26       1.00      0.88      0.93         8\n",
      "          27       1.00      0.25      0.40         4\n",
      "          28       0.57      0.40      0.47        10\n",
      "          29       0.57      1.00      0.73         4\n",
      "          30       0.88      0.58      0.70        12\n",
      "          31       0.78      0.54      0.64        13\n",
      "          32       1.00      0.80      0.89        10\n",
      "          33       0.80      0.80      0.80         5\n",
      "          34       0.75      0.43      0.55         7\n",
      "          35       1.00      0.33      0.50         6\n",
      "          36       0.36      0.36      0.36        11\n",
      "          37       0.50      0.50      0.50         2\n",
      "          38       0.50      0.33      0.40         3\n",
      "          39       0.00      0.00      0.00         5\n",
      "          40       0.75      0.30      0.43        10\n",
      "          41       0.83      0.62      0.71         8\n",
      "          42       1.00      1.00      1.00         3\n",
      "          43       0.75      1.00      0.86         6\n",
      "          44       0.67      0.80      0.73         5\n",
      "          45       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           0.81      2246\n",
      "   macro avg       0.71      0.62      0.64      2246\n",
      "weighted avg       0.80      0.81      0.80      2246\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        12\n",
      "           1       0.72      0.42      0.53       105\n",
      "           2       0.62      0.50      0.56        20\n",
      "           3       0.93      0.83      0.88       813\n",
      "           4       0.40      0.90      0.56       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       0.90      0.64      0.75        14\n",
      "           7       0.00      0.00      0.00         3\n",
      "           8       0.00      0.00      0.00        38\n",
      "           9       0.88      0.88      0.88        25\n",
      "          10       0.85      0.77      0.81        30\n",
      "          11       0.64      0.51      0.56        83\n",
      "          12       0.14      0.08      0.10        13\n",
      "          13       0.00      0.00      0.00        37\n",
      "          14       0.00      0.00      0.00         2\n",
      "          15       0.00      0.00      0.00         9\n",
      "          16       0.59      0.84      0.69        99\n",
      "          17       0.00      0.00      0.00        12\n",
      "          18       0.00      0.00      0.00        20\n",
      "          19       0.62      0.29      0.39       133\n",
      "          20       0.27      0.06      0.09        70\n",
      "          21       0.00      0.00      0.00        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.00      0.00      0.00        12\n",
      "          24       0.67      0.11      0.18        19\n",
      "          25       0.86      0.19      0.32        31\n",
      "          26       0.00      0.00      0.00         8\n",
      "          27       0.00      0.00      0.00         4\n",
      "          28       0.50      0.10      0.17        10\n",
      "          29       0.00      0.00      0.00         4\n",
      "          30       0.00      0.00      0.00        12\n",
      "          31       0.00      0.00      0.00        13\n",
      "          32       0.00      0.00      0.00        10\n",
      "          33       1.00      1.00      1.00         5\n",
      "          34       0.00      0.00      0.00         7\n",
      "          35       0.00      0.00      0.00         6\n",
      "          36       0.00      0.00      0.00        11\n",
      "          37       0.00      0.00      0.00         2\n",
      "          38       0.00      0.00      0.00         3\n",
      "          39       0.00      0.00      0.00         5\n",
      "          40       0.00      0.00      0.00        10\n",
      "          41       0.00      0.00      0.00         8\n",
      "          42       0.00      0.00      0.00         3\n",
      "          43       0.00      0.00      0.00         6\n",
      "          44       0.00      0.00      0.00         5\n",
      "          45       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.62      2246\n",
      "   macro avg       0.23      0.18      0.18      2246\n",
      "weighted avg       0.61      0.62      0.58      2246\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.22      0.33      0.27        12\n",
      "           1       0.45      0.77      0.57       105\n",
      "           2       0.30      0.30      0.30        20\n",
      "           3       0.82      0.90      0.86       813\n",
      "           4       0.61      0.83      0.70       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       0.67      0.43      0.52        14\n",
      "           7       0.50      0.33      0.40         3\n",
      "           8       0.67      0.53      0.59        38\n",
      "           9       0.70      0.28      0.40        25\n",
      "          10       0.75      0.30      0.43        30\n",
      "          11       0.55      0.59      0.57        83\n",
      "          12       0.40      0.15      0.22        13\n",
      "          13       0.37      0.19      0.25        37\n",
      "          14       0.00      0.00      0.00         2\n",
      "          15       0.00      0.00      0.00         9\n",
      "          16       0.59      0.59      0.59        99\n",
      "          17       0.00      0.00      0.00        12\n",
      "          18       0.50      0.25      0.33        20\n",
      "          19       0.69      0.54      0.61       133\n",
      "          20       0.57      0.29      0.38        70\n",
      "          21       0.67      0.30      0.41        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.00      0.00      0.00        12\n",
      "          24       0.67      0.11      0.18        19\n",
      "          25       1.00      0.32      0.49        31\n",
      "          26       1.00      0.25      0.40         8\n",
      "          27       0.00      0.00      0.00         4\n",
      "          28       0.00      0.00      0.00        10\n",
      "          29       0.00      0.00      0.00         4\n",
      "          30       0.50      0.08      0.14        12\n",
      "          31       0.67      0.15      0.25        13\n",
      "          32       0.67      0.20      0.31        10\n",
      "          33       1.00      0.60      0.75         5\n",
      "          34       0.50      0.14      0.22         7\n",
      "          35       1.00      0.17      0.29         6\n",
      "          36       0.33      0.09      0.14        11\n",
      "          37       0.00      0.00      0.00         2\n",
      "          38       0.00      0.00      0.00         3\n",
      "          39       0.00      0.00      0.00         5\n",
      "          40       1.00      0.20      0.33        10\n",
      "          41       0.00      0.00      0.00         8\n",
      "          42       0.00      0.00      0.00         3\n",
      "          43       0.67      0.33      0.44         6\n",
      "          44       1.00      0.80      0.89         5\n",
      "          45       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           0.67      2246\n",
      "   macro avg       0.46      0.27      0.31      2246\n",
      "weighted avg       0.66      0.67      0.64      2246\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.75      0.78        12\n",
      "           1       0.77      0.68      0.72       105\n",
      "           2       0.78      0.70      0.74        20\n",
      "           3       0.88      0.91      0.89       813\n",
      "           4       0.76      0.83      0.79       474\n",
      "           5       0.50      0.20      0.29         5\n",
      "           6       0.80      0.86      0.83        14\n",
      "           7       1.00      0.33      0.50         3\n",
      "           8       0.64      0.66      0.65        38\n",
      "           9       0.74      0.80      0.77        25\n",
      "          10       0.90      0.87      0.88        30\n",
      "          11       0.63      0.64      0.63        83\n",
      "          12       0.33      0.46      0.39        13\n",
      "          13       0.62      0.49      0.55        37\n",
      "          14       0.14      0.50      0.22         2\n",
      "          15       0.38      0.33      0.35         9\n",
      "          16       0.73      0.73      0.73        99\n",
      "          17       0.27      0.25      0.26        12\n",
      "          18       0.59      0.50      0.54        20\n",
      "          19       0.69      0.65      0.67       133\n",
      "          20       0.67      0.46      0.54        70\n",
      "          21       0.70      0.78      0.74        27\n",
      "          22       1.00      0.14      0.25         7\n",
      "          23       0.54      0.58      0.56        12\n",
      "          24       0.61      0.58      0.59        19\n",
      "          25       0.89      0.55      0.68        31\n",
      "          26       0.71      0.62      0.67         8\n",
      "          27       0.50      0.50      0.50         4\n",
      "          28       0.38      0.30      0.33        10\n",
      "          29       0.23      0.75      0.35         4\n",
      "          30       0.45      0.42      0.43        12\n",
      "          31       0.62      0.38      0.48        13\n",
      "          32       1.00      0.90      0.95        10\n",
      "          33       0.75      0.60      0.67         5\n",
      "          34       0.67      0.29      0.40         7\n",
      "          35       0.80      0.67      0.73         6\n",
      "          36       0.62      0.45      0.53        11\n",
      "          37       0.67      1.00      0.80         2\n",
      "          38       0.33      0.33      0.33         3\n",
      "          39       0.40      0.40      0.40         5\n",
      "          40       0.40      0.20      0.27        10\n",
      "          41       0.56      0.62      0.59         8\n",
      "          42       0.67      0.67      0.67         3\n",
      "          43       0.67      0.67      0.67         6\n",
      "          44       0.67      0.80      0.73         5\n",
      "          45       0.33      1.00      0.50         1\n",
      "\n",
      "    accuracy                           0.77      2246\n",
      "   macro avg       0.63      0.58      0.58      2246\n",
      "weighted avg       0.77      0.77      0.76      2246\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.75      0.82        12\n",
      "           1       0.77      0.74      0.76       105\n",
      "           2       0.73      0.80      0.76        20\n",
      "           3       0.92      0.94      0.93       813\n",
      "           4       0.83      0.88      0.85       474\n",
      "           5       1.00      0.20      0.33         5\n",
      "           6       0.86      0.86      0.86        14\n",
      "           7       1.00      0.67      0.80         3\n",
      "           8       0.70      0.68      0.69        38\n",
      "           9       0.81      0.84      0.82        25\n",
      "          10       0.93      0.90      0.92        30\n",
      "          11       0.65      0.69      0.67        83\n",
      "          12       0.46      0.46      0.46        13\n",
      "          13       0.68      0.62      0.65        37\n",
      "          14       0.14      0.50      0.22         2\n",
      "          15       0.57      0.44      0.50         9\n",
      "          16       0.72      0.75      0.73        99\n",
      "          17       0.53      0.67      0.59        12\n",
      "          18       0.79      0.55      0.65        20\n",
      "          19       0.68      0.69      0.68       133\n",
      "          20       0.62      0.49      0.54        70\n",
      "          21       0.65      0.81      0.72        27\n",
      "          22       1.00      0.14      0.25         7\n",
      "          23       0.60      0.75      0.67        12\n",
      "          24       0.67      0.63      0.65        19\n",
      "          25       0.95      0.68      0.79        31\n",
      "          26       0.88      0.88      0.88         8\n",
      "          27       0.67      0.50      0.57         4\n",
      "          28       0.44      0.40      0.42        10\n",
      "          29       0.40      1.00      0.57         4\n",
      "          30       0.67      0.50      0.57        12\n",
      "          31       0.75      0.46      0.57        13\n",
      "          32       1.00      1.00      1.00        10\n",
      "          33       0.80      0.80      0.80         5\n",
      "          34       0.75      0.43      0.55         7\n",
      "          35       1.00      0.67      0.80         6\n",
      "          36       0.56      0.45      0.50        11\n",
      "          37       0.67      1.00      0.80         2\n",
      "          38       0.50      0.33      0.40         3\n",
      "          39       0.50      0.40      0.44         5\n",
      "          40       0.67      0.20      0.31        10\n",
      "          41       0.80      0.50      0.62         8\n",
      "          42       0.75      1.00      0.86         3\n",
      "          43       0.67      1.00      0.80         6\n",
      "          44       0.67      0.80      0.73         5\n",
      "          45       0.50      1.00      0.67         1\n",
      "\n",
      "    accuracy                           0.81      2246\n",
      "   macro avg       0.71      0.66      0.66      2246\n",
      "weighted avg       0.81      0.81      0.81      2246\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "numword_10000_model = fitting_model(x_train_tfidfv10, \n",
    "                                    y_train10, \n",
    "                                    x_test_tfidfv10, \n",
    "                                    y_test10)\n",
    "print(numword_10000_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "transsexual-manner",
   "metadata": {},
   "source": [
    "# 4. 딥러닝 모델과 비교해 보기\n",
    "* 간단한 딥러닝 모델 돌려서 확인\n",
    "* 별도의 Word2Vec등의 Pretrained model은 사용하지 않도록 할 것"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "serious-rental",
   "metadata": {},
   "source": [
    "## 4.1 기존 데이터 재활용 및 LSTM 모델 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "atomic-chart",
   "metadata": {},
   "source": [
    "### 4.1.1 데이터 array 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "intense-formula",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8982, 4867) (2246, 4867)\n",
      "<class 'numpy.matrix'> <class 'tuple'>\n"
     ]
    }
   ],
   "source": [
    "X_train = x_train_tfidfv5.todense()\n",
    "X_test = x_test_tfidfv5.todense()\n",
    "\n",
    "print(X_train.shape, X_test.shape)\n",
    "print(type(X_train), type(X_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "through-snowboard",
   "metadata": {},
   "source": [
    "### 4.1.2 데이터 one hot encoding 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "vocal-meaning",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8982, 46) (2246, 46)\n",
      "<class 'numpy.ndarray'> <class 'tuple'>\n"
     ]
    }
   ],
   "source": [
    "y_train = to_categorical(y_train5)\n",
    "y_test = to_categorical(y_test5)\n",
    "\n",
    "print(y_train.shape, y_test.shape)\n",
    "print(type(y_train), type(y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hybrid-quebec",
   "metadata": {},
   "source": [
    "### 4.1.3 LSTM 정의 및 활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "static-newfoundland",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 100)         500000    \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 128)               117248    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 46)                5934      \n",
      "=================================================================\n",
      "Total params: 623,182\n",
      "Trainable params: 623,182\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vocab_size= 5000\n",
    "word_vector_dim =100\n",
    "\n",
    "# LSTM\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Embedding(vocab_size, word_vector_dim))\n",
    "model.add(keras.layers.LSTM(128))  \n",
    "model.add(keras.layers.Dense(46, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "afraid-glory",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early Stopping\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\n",
    "\n",
    "# Model Checkpoint\n",
    "mc = ModelCheckpoint('best_tf_model_LSTM.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "australian-belly",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "retained-roberts",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "71/71 [==============================] - 68s 921ms/step - loss: 2.9502 - acc: 0.2944 - val_loss: 2.4221 - val_acc: 0.3620\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.36198, saving model to best_tf_model_LSTM.h5\n",
      "Epoch 2/15\n",
      "71/71 [==============================] - 65s 911ms/step - loss: 2.4047 - acc: 0.3499 - val_loss: 2.4161 - val_acc: 0.3620\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.36198\n",
      "Epoch 3/15\n",
      "71/71 [==============================] - 65s 911ms/step - loss: 2.4042 - acc: 0.3502 - val_loss: 2.4211 - val_acc: 0.3620\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.36198\n",
      "Epoch 4/15\n",
      "71/71 [==============================] - 65s 912ms/step - loss: 2.3934 - acc: 0.3531 - val_loss: 2.4246 - val_acc: 0.3620\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.36198\n",
      "Epoch 5/15\n",
      "71/71 [==============================] - 65s 912ms/step - loss: 2.3821 - acc: 0.3510 - val_loss: 2.4216 - val_acc: 0.3620\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.36198\n",
      "Epoch 6/15\n",
      "71/71 [==============================] - 65s 913ms/step - loss: 2.3756 - acc: 0.3620 - val_loss: 2.4219 - val_acc: 0.3620\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.36198\n",
      "Epoch 00006: early stopping\n"
     ]
    }
   ],
   "source": [
    "# Model fit\n",
    "history = model.fit(X_train, y_train, batch_size=128, epochs=15, callbacks=[es, mc], validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "trying-monitor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71/71 [==============================] - 14s 194ms/step - loss: 2.4221 - acc: 0.3620\n",
      "LSTM eval:  0.36197686195373535\n"
     ]
    }
   ],
   "source": [
    "loaded_model = load_model('best_tf_model_LSTM.h5')\n",
    "print('LSTM eval: ', loaded_model.evaluate(X_test, y_test)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "closed-shadow",
   "metadata": {},
   "source": [
    "# 5. 설명"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "systematic-heath",
   "metadata": {},
   "source": [
    "* num_wards를 None으로 하기에는 시간이 너무 오래 걸렸다.\n",
    "* 앞서 알게된 Stacking ensemble이나 모델의 Hyperparameter 값을 조정한다면 ML의 성능을 조금 더 끌어 올릴 수 있을 것 같다.\n",
    "* 단순한 1 LSTM Layer로는 성능을 가늠하기 어렵다.   \n",
    "* num_words 3000과 5000을 비교해보면 성능이 별다른 차이가 없었다.\n",
    " * 3000 : 모델 중 Voting 최대 성능 81%\n",
    " * 5000 : 모델 중 Voting 최대 성능 82%\n",
    " * 10000 : 36%\n",
    "* Grid Search 및 Random한 방법으로 HyperParameter를 설정하고 catboost 및 stacking ensemble 모델을 사용한 뒤 성능이 좋은 모델끼리 Voting한다면 좋은 결과를 기대할 수 있을 것 같다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "significant-shopping",
   "metadata": {},
   "source": [
    "* 동일한 전처리로 수행한 결과 36%로 성능이 좋지 못했다.(TF-IDF 변경 후)\n",
    "* TF-IDF를 변경하지 않고 해야한다.\n",
    " * 시간이 없는 관계로 수행하지 못했다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "historical-taxation",
   "metadata": {},
   "source": [
    "# 6. 참고"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dimensional-restaurant",
   "metadata": {},
   "source": [
    "* [딥러닝을 이용한 자연어 처리 입문 : 로이터 뉴스 데이터](https://wikidocs.net/22933)\n",
    "* [딥러닝을 이용한 자연어 처리 입문 : 단어의 다양한 표현](https://wikidocs.net/31767)\n",
    "* [SciPy 압축 희소 행(CSR) 행렬을 NumPy 행렬로 변환하기 : todense](https://rfriend.tistory.com/551)\n",
    "* [Tensorflow keras : to_categorical](https://www.tensorflow.org/api_docs/python/tf/keras/utils/to_categorical)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
