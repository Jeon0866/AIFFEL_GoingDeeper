{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "wanted-sensitivity",
   "metadata": {},
   "source": [
    "# 멋진 챗봇 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "swedish-lover",
   "metadata": {},
   "source": [
    "|평가문항|상세기준|\n",
    "|---|---|\n",
    "|1. 챗봇 훈련데이터 전처리 과정이 체계적으로 진행되었는가?|챗봇 훈련데이터를 위한 전처리와 augmentation이 적절히 수행되어 3만개 가량의 훈련데이터셋이 구축되었다.|\n",
    "|2. transformer 모델을 활용한 챗봇 모델이 과적합을 피해 안정적으로 훈련되었는가?|과적합을 피할 수 있는 하이퍼파라미터 셋이 적절히 제시되었다.|\n",
    "|3. 챗봇이 사용자의 질문에 그럴듯한 형태로 답하는 사례가 있는가?|주어진 예문을 포함하여 챗봇에 던진 질문에 적절히 답하는 사례가 제출되었다.|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collaborative-thing",
   "metadata": {},
   "source": [
    "# 패키지 및 라이브러리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "viral-remark",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "import re\n",
    "import os\n",
    "import io\n",
    "import time\n",
    "import random\n",
    "\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm_notebook\n",
    "from collections import Counter\n",
    "from konlpy.tag import Mecab\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "derived-postage",
   "metadata": {},
   "source": [
    "# 데이터 다운로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "agreed-pulse",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data load\n",
    "path_to_file = os.getenv('HOME')+'/aiffel/transformer_chatbot/ChatbotData.csv'\n",
    "data = pd.read_csv(path_to_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "moderate-frontier",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q</th>\n",
       "      <th>A</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12시 땡!</td>\n",
       "      <td>하루가 또 가네요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1지망 학교 떨어졌어</td>\n",
       "      <td>위로해 드립니다.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3박4일 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3박4일 정도 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PPL 심하네</td>\n",
       "      <td>눈살이 찌푸려지죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11818</th>\n",
       "      <td>훔쳐보는 것도 눈치 보임.</td>\n",
       "      <td>티가 나니까 눈치가 보이는 거죠!</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11819</th>\n",
       "      <td>훔쳐보는 것도 눈치 보임.</td>\n",
       "      <td>훔쳐보는 거 티나나봐요.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11820</th>\n",
       "      <td>흑기사 해주는 짝남.</td>\n",
       "      <td>설렜겠어요.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11821</th>\n",
       "      <td>힘든 연애 좋은 연애라는게 무슨 차이일까?</td>\n",
       "      <td>잘 헤어질 수 있는 사이 여부인 거 같아요.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11822</th>\n",
       "      <td>힘들어서 결혼할까봐</td>\n",
       "      <td>도피성 결혼은 하지 않길 바라요.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11823 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Q                         A  label\n",
       "0                       12시 땡!                하루가 또 가네요.      0\n",
       "1                  1지망 학교 떨어졌어                 위로해 드립니다.      0\n",
       "2                 3박4일 놀러가고 싶다               여행은 언제나 좋죠.      0\n",
       "3              3박4일 정도 놀러가고 싶다               여행은 언제나 좋죠.      0\n",
       "4                      PPL 심하네                눈살이 찌푸려지죠.      0\n",
       "...                        ...                       ...    ...\n",
       "11818           훔쳐보는 것도 눈치 보임.        티가 나니까 눈치가 보이는 거죠!      2\n",
       "11819           훔쳐보는 것도 눈치 보임.             훔쳐보는 거 티나나봐요.      2\n",
       "11820              흑기사 해주는 짝남.                    설렜겠어요.      2\n",
       "11821  힘든 연애 좋은 연애라는게 무슨 차이일까?  잘 헤어질 수 있는 사이 여부인 거 같아요.      2\n",
       "11822               힘들어서 결혼할까봐        도피성 결혼은 하지 않길 바라요.      2\n",
       "\n",
       "[11823 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "revolutionary-favor",
   "metadata": {},
   "outputs": [],
   "source": [
    "src = []\n",
    "tgt = []\n",
    "for s, t in zip(data['Q'], data['A']):\n",
    "    src.append(str(s))\n",
    "    tgt.append(str(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "anonymous-painting",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q:  12시 땡! \n",
      " Q 총 길이:  11823\n",
      "A:  하루가 또 가네요. \n",
      " A 총 길이:  11823\n"
     ]
    }
   ],
   "source": [
    "print('Q: ', src[0], '\\n','Q 총 길이: ', len(src))\n",
    "print('A: ', tgt[0], '\\n','A 총 길이: ', len(tgt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "experienced-alaska",
   "metadata": {},
   "source": [
    "# 데이터 정제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "exposed-robert",
   "metadata": {},
   "outputs": [],
   "source": [
    "mecab = Mecab()\n",
    "\n",
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip()\n",
    "\n",
    "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "    sentence = re.sub(r\"[^0-9ㄱ-ㅎㅏ-ㅣ가-힣a-zA-Z?.!,]+\", \" \", sentence)\n",
    "\n",
    "    sentence = sentence.strip()\n",
    "    corpus = mecab.morphs(sentence)\n",
    "    \n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "earned-croatia",
   "metadata": {},
   "source": [
    "# 데이터 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "handled-mozambique",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_corpus = list(set(zip(src,tgt)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "surface-kitty",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11750\n",
      "11750\n",
      "Questions ['언젠간', '먼저', '연락', '오', '겠', '죠', '?']\n",
      "Answers: ['붙잡', '고', '싶', '다면', '먼저', '연락', '해', '보', '세요', '.']\n"
     ]
    }
   ],
   "source": [
    "src_corpus = []\n",
    "tgt_corpus = []\n",
    "\n",
    "\n",
    "for tmp in cleaned_corpus:\n",
    "    #print(tmp[0])\n",
    "    #print(tmp[1])\n",
    "    tmp_src = preprocess_sentence(tmp[0])\n",
    "    tmp_tgt = preprocess_sentence(tmp[1])\n",
    "    #if len(tmp_ko) <= 40:\n",
    "    src_corpus.append(tmp_src)\n",
    "    tgt_corpus.append(tmp_tgt)\n",
    "\n",
    "    \n",
    "que_corpus = src_corpus\n",
    "ans_corpus = tgt_corpus    \n",
    "\n",
    "\n",
    "print(len(src_corpus))\n",
    "print(len(tgt_corpus))\n",
    "print(\"Questions\", src_corpus[100])   \n",
    "print(\"Answers:\", tgt_corpus[100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "judicial-affiliation",
   "metadata": {},
   "source": [
    "# Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "particular-uniform",
   "metadata": {},
   "source": [
    "! pip install --upgrade gensim==3.8.3\n",
    "\n",
    "* gensim 버전을 변경하지 않으면 아래 Word2Vec.load가 실행되지 않음. \n",
    " * https://iambeginnerdeveloper.tistory.com/41"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "architectural-singing",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_path = os.getenv('HOME') + '/aiffel/transformer_chatbot/ko.bin'\n",
    "word2vec = Word2Vec.load(word2vec_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "pressed-privilege",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `similar_by_word` (Method will be removed in 4.0.0, use self.wv.similar_by_word() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('연인', 0.7585031986236572),\n",
       " ('애인', 0.7489752769470215),\n",
       " ('엄마', 0.7044110298156738),\n",
       " ('동료', 0.6891161799430847),\n",
       " ('언니', 0.6841583251953125),\n",
       " ('후배', 0.6707336902618408),\n",
       " ('오빠', 0.6663583517074585),\n",
       " ('하녀', 0.6528609395027161),\n",
       " ('선배', 0.6471554040908813),\n",
       " ('절친', 0.6467020511627197)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec.similar_by_word(\"친구\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "focused-basin",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lexical Substitution 구현하기\n",
    "def lexical_sub(sentence, word2vec):\n",
    "    import random\n",
    "\n",
    "    res = \"\"\n",
    "    toks = sentence\n",
    "\n",
    "    try:\n",
    "        _from = random.choice(toks)\n",
    "        _to = word2vec.most_similar(_from)[0][0]\n",
    "\n",
    "    except:   # 단어장에 없는 단어|\n",
    "        return None\n",
    "\n",
    "    for tok in toks:\n",
    "        if tok is _from: res += _to + \" \"\n",
    "        else: res += tok + \" \"\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "widespread-marketing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['언젠간', '먼저', '연락', '오', '겠', '죠', '?']\n",
      "종교 그리고 이 별 그리고 헤어지 . \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:10: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "print(que_corpus[100])\n",
    "print(lexical_sub(que_corpus[1], word2vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "committed-locator",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:5: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "caf44b9d53284b9fb1519d89059fdacd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11750 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:10: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:17: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a2cea2ff57546b095908b335c2fcfd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11750 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "new_que_corpus = []\n",
    "new_ans_corpus = []\n",
    "\n",
    "# Augmentation된 que_corpus 와 원본 ans_corpus 가 병렬을 이루도록 한다.\n",
    "for idx in tqdm_notebook(range(len(que_corpus))):\n",
    "    que_augmented = lexical_sub(que_corpus[idx], word2vec)\n",
    "    ans = ans_corpus[idx]\n",
    "    \n",
    "    if que_augmented is not None:\n",
    "        new_que_corpus.append(que_augmented.split())\n",
    "        new_ans_corpus.append(ans)\n",
    "        \n",
    "    else:\n",
    "       \n",
    "        continue\n",
    "    \n",
    "for idx in tqdm_notebook(range(len(ans_corpus))):\n",
    "    que = que_corpus[idx]\n",
    "    ans_augmented = lexical_sub(ans_corpus[idx], word2vec)\n",
    "    \n",
    "    if ans_augmented is not None:\n",
    "        new_que_corpus.append(que)\n",
    "        new_ans_corpus.append(ans_augmented.split())\n",
    "       \n",
    "    else:\n",
    "       \n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "danish-subscriber",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['또', '폭식', '했', '다', '는데'], ['종교', '그리고', '이', '항성', '그리고', '재회', '.'], ['에드', '진짜', '쓰레기', '야'], ['화', '를', '한', '번', '내고', '는', '게', '나을', '텐데'], ['고집', '하', '고서'], ['오늘', '화장', '이', '안', '씹', '어'], ['짝사랑', '은', '힘든', '거', '같', '아서'], ['이', '별후', '에', '때문', '나', '를', '괴롭히', '는', '그', '의', '말', '들'], ['이뤄질', '수', '없', 'ㄴ다는', '사랑', '이', '제일', '서글퍼', '.'], ['친구', '카나', '놀', '러', '가', '려고']]\n",
      "[['나', '를', '관찰', '하', '고', '음식', '자체', '에', '집중', '하', '세요', '.'], ['종교', '문제', '가', '힘들', '죠', '.'], ['욕해', '주', '세요', '.'], ['화', '내', '는', '연습', '을', '해', '보', '세요', '.'], ['피할', '수', '있', '으면', '피하', '세요', '.'], ['각질', '제거', '를', '해', '보', '세요', '.'], ['그래도', '좋', '아', '하', '는', '동안', '행복', '하', '길', '바라', '요', '.'], ['이별', '은', '상처', '를', '남겨서', '아무', '는', '데', '는', '시간', '이', '거릴', '거', '예요', '.'], ['알', '면서', '도', '사랑', '할', '수', '밖', '에', '없', '어서', '그런', '것', '같', '아요', '.'], ['재미있', '게', '놀', '다', '오', '세요', '.']]\n",
      "--------------------------------------------------------------------------------\n",
      "20272\n",
      "20272\n"
     ]
    }
   ],
   "source": [
    "print(new_que_corpus[:10])\n",
    "print(new_ans_corpus[:10])\n",
    "print('-'*80)\n",
    "print(len(new_que_corpus))\n",
    "print(len(new_ans_corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daily-journalism",
   "metadata": {},
   "source": [
    "# 데이터 벡터화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "royal-separate",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<start>', '나', '를', '관찰', '하', '고', '음식', '자체', '에', '집중', '하', '세요', '.', '<end>']\n",
      "['<start>', '시간', '이', '도와', '줄', '거', '예요', '.', '<end>']\n",
      "['<start>', '분명', '나아질', '거', '예요', '.', '<end>']\n"
     ]
    }
   ],
   "source": [
    "tgt_corpus = []\n",
    "\n",
    "for corpus in ans_corpus:\n",
    "    tgt_corpus.append([\"<start>\"] + corpus + [\"<end>\"])\n",
    "    \n",
    "print(tgt_corpus[0])\n",
    "print(tgt_corpus[325])\n",
    "print(tgt_corpus[395])\n",
    "ans_corpus = tgt_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "classical-aberdeen",
   "metadata": {},
   "outputs": [],
   "source": [
    "voc_data = que_corpus + ans_corpus\n",
    "\n",
    "words = np.concatenate(voc_data).tolist()\n",
    "counter = Counter(words)\n",
    "counter = counter.most_common(30000-2)\n",
    "vocab = ['<pad>', '<unk>'] + [key for key, _ in counter]\n",
    "word_to_index = {word:index for index, word in enumerate(vocab)}\n",
    "index_to_word = {index:word for word, index in word_to_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fitted-mining",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11750\n",
      "11750\n"
     ]
    }
   ],
   "source": [
    "def get_encoded_sentence(sentence, word_to_index):\n",
    "    return [word_to_index[word] if word in word_to_index else word_to_index['<unk>'] for word in sentence]\n",
    "\n",
    "def get_decoded_sentence(encoded_sentence, index_to_word):\n",
    "    return ' '.join(index_to_word[index] if index in index_to_word else '<unk>' for index in encoded_sentence[1:])  #[1:]를 통해 <BOS>를 제외\n",
    "\n",
    "def vectorize(corpus, word_to_index):\n",
    "    data = []\n",
    "    for sen in corpus:\n",
    "        sen = get_encoded_sentence(sen, word_to_index)\n",
    "        data.append(sen)\n",
    "    return data\n",
    "\n",
    "que_train = vectorize(que_corpus, word_to_index)\n",
    "ans_train = vectorize(ans_corpus, word_to_index)\n",
    "\n",
    "print(len(que_train))\n",
    "print(len(ans_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "effective-carolina",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11632\n",
      "118\n",
      "11632\n",
      "118\n"
     ]
    }
   ],
   "source": [
    "enc_tensor = tf.keras.preprocessing.sequence.pad_sequences(que_train, padding='post')\n",
    "dec_tensor = tf.keras.preprocessing.sequence.pad_sequences(ans_train, padding='post')\n",
    "\n",
    "enc_train, enc_val, dec_train, dec_val = \\\n",
    "train_test_split(enc_tensor, dec_tensor, test_size=0.01) # test set은 1%만\n",
    "\n",
    "print(len(enc_train))\n",
    "print(len(enc_val)) \n",
    "print(len(dec_train))\n",
    "print(len(dec_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confirmed-clearance",
   "metadata": {},
   "source": [
    "# Transfomer 모델링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "molecular-albert",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positional Encoding 구현\n",
    "def positional_encoding(pos, d_model):\n",
    "    def cal_angle(position, i):\n",
    "        return position / np.power(10000, int(i) / d_model)\n",
    "\n",
    "    def get_posi_angle_vec(position):\n",
    "        return [cal_angle(position, i) for i in range(d_model)]\n",
    "\n",
    "    sinusoid_table = np.array([get_posi_angle_vec(pos_i) for pos_i in range(pos)])\n",
    "\n",
    "    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])  # 인덱스가 짝수\n",
    "    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])  # 인덱스가 홀수\n",
    "\n",
    "             \n",
    "    return sinusoid_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "mental-enemy",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Mask  생성하기\n",
    "def generate_padding_mask(seq):\n",
    "    # tf.math.equal: seq의 원소가 0이 되면 true로 반환, 아니면 false 반환\n",
    "    # tf.cast: true를 float32로 변환\n",
    "    \n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]    # np.newaxis: numpy array의 차원 늘려주기\n",
    "\n",
    "def generate_causality_mask(src_len, tgt_len):\n",
    "    # np.cumsum(): 배열에서 행에 따라 누적되는 원소들의 누적합 계산\n",
    "    # np.eye(): 대각선이 1인 seq_len x seq_len 크기의 대각행렬 생성\n",
    "    \n",
    "    mask = 1 - np.cumsum(np.eye(src_len, tgt_len), 0)\n",
    "    return tf.cast(mask, tf.float32)  # tf.cast: mask(텐서)를 float32로 변환\n",
    "\n",
    "\n",
    "def generate_masks(src, tgt):\n",
    "    enc_mask = generate_padding_mask(src)\n",
    "    dec_mask = generate_padding_mask(tgt)\n",
    "\n",
    "    dec_causality_mask = generate_causality_mask(tgt.shape[1], tgt.shape[1])\n",
    "    dec_mask = tf.maximum(dec_mask, dec_causality_mask)\n",
    "\n",
    "    dec_enc_causality_mask = generate_causality_mask(tgt.shape[1], src.shape[1])\n",
    "    dec_enc_mask = tf.maximum(enc_mask, dec_enc_causality_mask)\n",
    "\n",
    "    return enc_mask, dec_enc_mask, dec_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "northern-madness",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi Head Attention 구현\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.depth = d_model // self.num_heads\n",
    "        \n",
    "        # Linear Layer\n",
    "        self.W_q = tf.keras.layers.Dense(d_model)\n",
    "        self.W_k = tf.keras.layers.Dense(d_model)\n",
    "        self.W_v = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "        self.linear = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask):\n",
    "        d_k = tf.cast(K.shape[-1], tf.float32)\n",
    "        QK = tf.matmul(Q, K, transpose_b=True)\n",
    "\n",
    "        scaled_qk = QK / tf.math.sqrt(d_k)   #  Scaled QK\n",
    "\n",
    "        if mask is not None: scaled_qk += (mask * -1e9)  \n",
    "\n",
    "        attentions = tf.nn.softmax(scaled_qk, axis=-1)   # Attention Weights\n",
    "        out = tf.matmul(attentions, V)\n",
    "\n",
    "        return out, attentions\n",
    "\n",
    "\n",
    "    def split_heads(self, x):\n",
    "        bsz = x.shape[0]  # batch size\n",
    "        # reshape - shape의 한 원소만 -1, 의미는 전체 크기가 일정하게 유지되도록 해당 차원의 길이가 자동으로 계산\n",
    "        split_x = tf.reshape(x, (bsz, -1, self.num_heads, self.depth))\n",
    "        split_x = tf.transpose(split_x, perm=[0, 2, 1, 3])   #  perm은 치환하는 위치를 알려줌\n",
    "\n",
    "        return split_x\n",
    "\n",
    "    \n",
    "    def combine_heads(self, x):\n",
    "        bsz = x.shape[0]\n",
    "        combined_x = tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "        combined_x = tf.reshape(combined_x, (bsz, -1, self.d_model))\n",
    "\n",
    "        return combined_x\n",
    "\n",
    "\n",
    "    def call(self, Q, K, V, mask):\n",
    "        # Linear 레이어 추가 - embedding 매핑\n",
    "        WQ = self.W_q(Q)\n",
    "        WK = self.W_k(K)\n",
    "        WV = self.W_v(V)\n",
    "\n",
    "        WQ_splits = self.split_heads(WQ)\n",
    "        WK_splits = self.split_heads(WK)\n",
    "        WV_splits = self.split_heads(WV)\n",
    "\n",
    "        out, attention_weights = self.scaled_dot_product_attention(\n",
    "            WQ_splits, WK_splits, WV_splits, mask)\n",
    "\n",
    "        out = self.combine_heads(out)\n",
    "        out = self.linear(out)\n",
    "\n",
    "        return out, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "possible-ballot",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Position-wise Feed Forward Network 구현\n",
    "class PoswiseFeedForwardNet(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PoswiseFeedForwardNet, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_ff = d_ff\n",
    "\n",
    "        self.fc1 = tf.keras.layers.Dense(d_ff, activation='relu')\n",
    "        self.fc2 = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def call(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.fc2(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "recognized-qatar",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.enc_self_attn = MultiHeadAttention(d_model, n_heads)\n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "\n",
    "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "\n",
    "    def call(self, x, mask):\n",
    "\n",
    "        \"\"\"\n",
    "        Multi-Head Attention\n",
    "        \"\"\"\n",
    "        residual = x\n",
    "        out = self.norm_1(x)\n",
    "        out, enc_attn = self.enc_self_attn(out, out, out, mask)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "\n",
    "        \"\"\"\n",
    "        Position-Wise Feed Forward Network\n",
    "        \"\"\"\n",
    "        residual = out\n",
    "        out = self.norm_2(out)\n",
    "        out = self.ffn(out)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "\n",
    "        return out, enc_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "parental-cursor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder 레이어 구현\n",
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.dec_self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.enc_dec_attn = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "\n",
    "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "\n",
    "    def call(self, x, enc_out, causality_mask, padding_mask):\n",
    "\n",
    "        \"\"\"\n",
    "        Masked Multi-Head Attention\n",
    "        \"\"\"\n",
    "        residual = x\n",
    "        out = self.norm_1(x)\n",
    "        out, dec_attn = self.dec_self_attn(out, out, out, padding_mask)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "\n",
    "        \"\"\"\n",
    "        Multi-Head Attention\n",
    "        \"\"\"\n",
    "        residual = out\n",
    "        out = self.norm_2(out)\n",
    "        out, dec_enc_attn = self.dec_self_attn(out, enc_out, enc_out, causality_mask)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "\n",
    "        \"\"\"\n",
    "        Position-Wise Feed Forward Network\n",
    "        \"\"\"\n",
    "        residual = out\n",
    "        out = self.norm_3(out)\n",
    "        out = self.ffn(out)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "\n",
    "        return out, dec_attn, dec_enc_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "painful-jerusalem",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder 구현\n",
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                    n_layers,\n",
    "                    d_model,\n",
    "                    n_heads,\n",
    "                    d_ff,\n",
    "                    dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.enc_layers = [EncoderLayer(d_model, n_heads, d_ff, dropout) \n",
    "                        for _ in range(n_layers)]\n",
    "\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "\n",
    "    def call(self, x, mask):\n",
    "        out = x\n",
    "\n",
    "        enc_attns = list()\n",
    "        for i in range(self.n_layers):\n",
    "            out, enc_attn = self.enc_layers[i](out, mask)\n",
    "            enc_attns.append(enc_attn)\n",
    "\n",
    "        return out, enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "personalized-portugal",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder 구현\n",
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                    n_layers,\n",
    "                    d_model,\n",
    "                    n_heads,\n",
    "                    d_ff,\n",
    "                    dropout):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.dec_layers = [DecoderLayer(d_model, n_heads, d_ff, dropout) \n",
    "                            for _ in range(n_layers)]\n",
    "\n",
    "\n",
    "    def call(self, x, enc_out, causality_mask, padding_mask):\n",
    "        out = x\n",
    "\n",
    "        dec_attns = list()\n",
    "        dec_enc_attns = list()\n",
    "        for i in range(self.n_layers):\n",
    "            out, dec_attn, dec_enc_attn = \\\n",
    "            self.dec_layers[i](out, enc_out, causality_mask, padding_mask)\n",
    "\n",
    "            dec_attns.append(dec_attn)\n",
    "            dec_enc_attns.append(dec_enc_attn)\n",
    "\n",
    "        return out, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "phantom-private",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                    n_layers,\n",
    "                    d_model,\n",
    "                    n_heads,\n",
    "                    d_ff,  # 레이어의 차원수\n",
    "                    src_vocab_size,\n",
    "                    tgt_vocab_size,\n",
    "                    pos_len,\n",
    "                    dropout=0.2,\n",
    "                    shared_fc=True,\n",
    "                    shared_emb=False):\n",
    "        super(Transformer, self).__init__()\n",
    "    \n",
    "        self.d_model = tf.cast(d_model, tf.float32)\n",
    "\n",
    "        if shared_emb:\n",
    "            self.enc_emb = self.dec_emb = \\\n",
    "            tf.keras.layers.Embedding(src_vocab_size, d_model)\n",
    "        else:\n",
    "            self.enc_emb = tf.keras.layers.Embedding(src_vocab_size, d_model)\n",
    "            self.dec_emb = tf.keras.layers.Embedding(tgt_vocab_size, d_model)\n",
    "\n",
    "        self.pos_encoding = positional_encoding(pos_len, d_model)\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "\n",
    "        self.encoder = Encoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
    "        self.decoder = Decoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
    "\n",
    "        self.fc = tf.keras.layers.Dense(tgt_vocab_size)\n",
    "\n",
    "        self.shared_fc = shared_fc\n",
    "\n",
    "        if shared_fc:\n",
    "            self.fc.set_weights(tf.transpose(self.dec_emb.weights))\n",
    "\n",
    "    def embedding(self, emb, x):\n",
    "  \n",
    "        seq_len = x.shape[1]\n",
    "\n",
    "        out = emb(x)\n",
    "\n",
    "        if self.shared_fc: out *= tf.math.sqrt(self.d_model)\n",
    "\n",
    "        out += self.pos_encoding[np.newaxis, ...][:, :seq_len, :]\n",
    "        # np.newaxis: numpy array의 차원 늘려주기\n",
    "        \n",
    "        out = self.do(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "    def call(self, enc_in, dec_in, enc_mask, causality_mask, dec_mask):\n",
    "        \n",
    "        enc_in = self.embedding(self.enc_emb, enc_in)\n",
    "        dec_in = self.embedding(self.dec_emb, dec_in)\n",
    "\n",
    "        enc_out, enc_attns = self.encoder(enc_in, enc_mask)\n",
    "\n",
    "        dec_out, dec_attns, dec_enc_attns = \\\n",
    "        self.decoder(dec_in, enc_out, causality_mask, dec_mask)\n",
    "\n",
    "        logits = self.fc(dec_out)\n",
    "\n",
    "        return logits, enc_attns, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "searching-cassette",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 주어진 하이퍼파라미터로 Transformer 인스턴스 생성\n",
    "transformer = Transformer(\n",
    "    n_layers=4,\n",
    "    d_model=128,\n",
    "    n_heads=8,\n",
    "    d_ff=256,\n",
    "    src_vocab_size=30000,\n",
    "    tgt_vocab_size=30000,\n",
    "    pos_len=42,\n",
    "    dropout=0.3,\n",
    "    shared_fc=True,\n",
    "    shared_emb=True)\n",
    "\n",
    "d_model = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "separated-walker",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearningRateScheduler(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(LearningRateScheduler, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        arg1 = step ** -0.5\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "        return (self.d_model ** -0.5) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "laughing-geneva",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning Rate 인스턴스 선언\n",
    "learning_rate = LearningRateScheduler(d_model)\n",
    "\n",
    "# Optimizer 구현\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate,\n",
    "                                        beta_1=0.9,\n",
    "                                        beta_2=0.98, \n",
    "                                        epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "heard-elder",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Function 정의\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "appreciated-prison",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Step 정의\n",
    "@tf.function()\n",
    "def train_step(src, tgt, model, optimizer):\n",
    "    tgt_in = tgt[:, :-1]  # Decoder의 input\n",
    "    gold = tgt[:, 1:]     # Decoder의 output과 비교하기 위해 right shift를 통해 생성한 최종 타겟\n",
    "\n",
    "    enc_mask, dec_enc_mask, dec_mask = generate_masks(src, tgt_in)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "        model(src, tgt_in, enc_mask, dec_enc_mask, dec_mask)\n",
    "        loss = loss_function(gold, predictions)\n",
    "\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)    \n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    return loss, enc_attns, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "missing-columbia",
   "metadata": {},
   "source": [
    "# 훈련하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "romantic-beaver",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(sentence, model):\n",
    "    # sentence 전처리(enc_train과 같은 모양으로)\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "    pieces = sentence\n",
    "    tokens = get_encoded_sentence(pieces, word_to_index)\n",
    "\n",
    "    _input = tf.keras.preprocessing.sequence.pad_sequences([tokens],\n",
    "                                                           maxlen=enc_train.shape[-1],\n",
    "                                                           padding='post')\n",
    "    \n",
    "    ids = []\n",
    "    \n",
    "    output = tf.expand_dims([word_to_index[\"<start>\"]], 0) \n",
    "    for i in range(dec_train.shape[-1]):\n",
    "        enc_padding_mask, combined_mask, dec_padding_mask = \\\n",
    "        generate_masks(_input, output)\n",
    "\n",
    "        predictions, enc_attns, dec_attns, dec_enc_attns =\\\n",
    "        model(_input, \n",
    "              output,\n",
    "              enc_padding_mask,\n",
    "              combined_mask,\n",
    "              dec_padding_mask)\n",
    "\n",
    "        predicted_id = \\\n",
    "        tf.argmax(tf.math.softmax(predictions, axis=-1)[0, -1]).numpy().item()\n",
    "        \n",
    "        # 예측 단어가 종료 토큰일 경우\n",
    "        if word_to_index[\"<end>\"] == predicted_id:\n",
    "            result = get_decoded_sentence(ids, index_to_word)\n",
    "            return pieces, result, enc_attns, dec_attns, dec_enc_attns\n",
    "        ##word_to_index\n",
    "        ids.append(predicted_id)\n",
    "        output = tf.concat([output, tf.expand_dims([predicted_id], 0)], axis=-1)\n",
    "\n",
    "    result = get_decoded_sentence(ids, index_to_word)\n",
    "\n",
    "    return pieces, result, enc_attns, dec_attns, dec_enc_attns\n",
    "\n",
    "def translate(sentence, model):\n",
    "    pieces, result, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "    evaluate(sentence, model)\n",
    "    \n",
    "    print('Input: %s' % (sentence))\n",
    "    print('Predicted translation: {}'.format(result))\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "great-trust",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:9: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33d43fee0ff14f8bb3d58fb6a5a8d116",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/182 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4f2840f31c84d8ca5b7e8e32cb57845",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/182 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f26fce596248490f91f5a60f0b57a3c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/182 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8fb620485ec406586cd1952ecd402cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/182 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf65501fb52149b2aa317a577e266a66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/182 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a83c7cb0ce974010bd90b461ee001c10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/182 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1530d45cd1bb46aa9de77300d1226782",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/182 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81bc8757797a4b40aef595105c8d5fae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/182 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fe2fdf63c82458ba68ce5752a41a44e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/182 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb6876e3b1de473b84706715a8d0cb69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/182 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3db0a41174174fee9bf5d155c57a7b74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/182 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a22321f36c24472199c37a59aeabc486",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/182 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6883a3d015b4ddaa373987cb83d0b76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/182 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8d0a2baefec4bdb8b453a0e815786b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/182 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15d670af69a647d891cc67bc8a88eb46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/182 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3fb03573c034ec38635358cbd88f4c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/182 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39005cc06e8948cd8e83b47eb9e81121",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/182 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c5cd629cd4a48af9c29fdb9b3329557",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/182 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb98307a6fbc4f40aa2920fe1e141845",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/182 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e189230b1fe349c9984360cb67bfca6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/182 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f698272da384993b95e56ac50acaa5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/182 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c21b55cbb3643ac95ef68bc8692e46c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/182 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "333266203d2b459cb3d9a0adab7951ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/182 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6276cb16c40415a907f4aacb57c610c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/182 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60728188d0104f15ace8e6d079e3902b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/182 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8719a91d6597429bb8a2cae83d312eb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/182 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "987064dbd5084b179c3af34dac01966c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/182 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbb5410a15184d7b9206ef95b34fea87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/182 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcc8bafa7ae74ad4b11bd4f43d9d7408",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/182 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "973c89545c3c49d1b5d5ae7ae33d5724",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/182 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "EPOCHS = 30\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "\n",
    "    idx_list = list(range(0, enc_train.shape[0], BATCH_SIZE))\n",
    "    random.shuffle(idx_list)\n",
    "    t = tqdm_notebook(idx_list)\n",
    "\n",
    "    for (batch, idx) in enumerate(t):\n",
    "        batch_loss, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "        train_step(enc_train[idx:idx+BATCH_SIZE],\n",
    "                    dec_train[idx:idx+BATCH_SIZE],\n",
    "                    transformer,\n",
    "                    optimizer)\n",
    "\n",
    "        total_loss += batch_loss\n",
    "\n",
    "        t.set_description_str('Epoch %2d' % (epoch + 1))\n",
    "        t.set_postfix_str('Loss %.4f' % (total_loss.numpy() / (batch + 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "turned-senator",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    \"지루하다, 놀러가고 싶어.\",\n",
    "    \"오늘 일찍 일어났더니 피곤하다.\",\n",
    "    \"간만에 여자친구랑 데이트 하기로 했어.\",\n",
    "    \"집에 있는다는 소리야.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "existing-marijuana",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 지루하다, 놀러가고 싶어.\n",
      "Predicted translation: 때 도 아 가능 해요 .\n",
      "Input: 오늘 일찍 일어났더니 피곤하다.\n",
      "Predicted translation: 타 는 건 이겨야 해질 거 예요 .\n",
      "Input: 간만에 여자친구랑 데이트 하기로 했어.\n",
      "Predicted translation: 봤 네요 .\n",
      "Input: 집에 있는다는 소리야.\n",
      "Predicted translation: 은 죽 은가 봐요 .\n"
     ]
    }
   ],
   "source": [
    "for example in examples:\n",
    "    translate(example, transformer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thorough-supply",
   "metadata": {},
   "source": [
    "# 성능 측정하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "significant-touch",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원문: ['자연어', '처리에서', '트랜스포머는', '꼭', '자세히', '알고', '지나가야한다.']\n",
      "번역문: ['자', '연어', '처리의', '트랜스포머는', '자세히', '알고', '가야', '된다.']\n",
      "BLEU Score: 7.176381577237209e-155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/nltk/translate/bleu_score.py:516: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/opt/conda/lib/python3.7/site-packages/nltk/translate/bleu_score.py:516: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "reference = \"자연어 처리에서 트랜스포머는 꼭 자세히 알고 지나가야한다.\".split()\n",
    "candidate = \"자 연어 처리의 트랜스포머는 자세히 알고 가야 된다.\".split()\n",
    "\n",
    "print(\"원문:\", reference)\n",
    "print(\"번역문:\", candidate)\n",
    "print(\"BLEU Score:\", sentence_bleu([reference], candidate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "educated-selling",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-1: 0.375\n",
      "BLEU-2: 0.14285714285714285\n",
      "BLEU-3: 0.01666666666666667\n",
      "BLEU-4: 0.02\n",
      "\n",
      "BLEU-Total: 0.06500593260343691\n"
     ]
    }
   ],
   "source": [
    "def calculate_bleu(reference, candidate, weights=[0.25, 0.25, 0.25, 0.25]):\n",
    "    return sentence_bleu([reference],\n",
    "                         candidate,\n",
    "                         weights=weights,\n",
    "                         smoothing_function=SmoothingFunction().method1)  # smoothing_function 적용\n",
    "\n",
    "print(\"BLEU-1:\", calculate_bleu(reference, candidate, weights=[1, 0, 0, 0]))\n",
    "print(\"BLEU-2:\", calculate_bleu(reference, candidate, weights=[0, 1, 0, 0]))\n",
    "print(\"BLEU-3:\", calculate_bleu(reference, candidate, weights=[0, 0, 1, 0]))\n",
    "print(\"BLEU-4:\", calculate_bleu(reference, candidate, weights=[0, 0, 0, 1]))\n",
    "\n",
    "print(\"\\nBLEU-Total:\", calculate_bleu(reference, candidate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "processed-automation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_bleu(src_corpus, tgt_corpus, verbose=True):\n",
    "    total_score = 0.0\n",
    "    sample_size = len(tgt_corpus)\n",
    "\n",
    "    for idx in tqdm_notebook(range(sample_size)):\n",
    "        src_tokens = src_corpus[idx]\n",
    "        tgt_tokens = tgt_corpus[idx]\n",
    "        \n",
    "        src = []\n",
    "        tgt = []\n",
    "        \n",
    "        for word in src_tokens:\n",
    "            if word !=0 and word !=1 and word !=3 and word !=4:\n",
    "                src.append(word)\n",
    "        \n",
    "        for word in tgt_tokens:\n",
    "            if word != 0 and word != 3 and word !=4:\n",
    "                tgt.append(word)\n",
    "\n",
    "        src_sentence = get_decoded_sentence(src, index_to_word)\n",
    "        tgt_sentence = get_decoded_sentence(tgt, index_to_word)\n",
    "        \n",
    "        \n",
    "        reference = preprocess_sentence(tgt_sentence)\n",
    "        candidate = translate(src_sentence, transformer)\n",
    "\n",
    "        score = sentence_bleu([reference], candidate,\n",
    "                              smoothing_function=SmoothingFunction().method1)\n",
    "        total_score += score\n",
    "\n",
    "        if verbose:\n",
    "            print(\"Source Sentence: \", src_sentence)\n",
    "            print(\"Model Prediction: \", candidate)\n",
    "            print(\"Real: \", reference)\n",
    "            print(\"Score: %lf\\n\" % score)\n",
    "\n",
    "    print(\"Num of Sample:\", sample_size)\n",
    "    print(\"Total Score:\", total_score / sample_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "confused-scene",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:5: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "887efc0c23924514b41c5ddfa2e2b26f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 생활 해야 되 는 거 싫 다\n",
      "Predicted translation: 하 면 더 힘들 죠 .\n",
      "Source Sentence:  생활 해야 되 는 거 싫 다\n",
      "Model Prediction:  하 면 더 힘들 죠 .\n",
      "Real:  ['적응', '될', '거', '예요', '.']\n",
      "Score: 0.017033\n",
      "\n",
      "Input: 쉬 면서 애기 키워야 할 듯\n",
      "Predicted translation: 곳 과 데이트 를 찾 기 도 해요 .\n",
      "Source Sentence:  쉬 면서 애기 키워야 할 듯\n",
      "Model Prediction:  곳 과 데이트 를 찾 기 도 해요 .\n",
      "Real:  ['은', '곳', '에서', '일', '하', '시', '나', '봐요', '.']\n",
      "Score: 0.011452\n",
      "\n",
      "Input: 를 사랑 한 그 사람 에게 해 줄 수 있 는 것\n",
      "Predicted translation: 수 있 어요 .\n",
      "Source Sentence:  를 사랑 한 그 사람 에게 해 줄 수 있 는 것\n",
      "Model Prediction:  수 있 어요 .\n",
      "Real:  ['잊', '어', '가', '요', '.']\n",
      "Score: 0.036556\n",
      "\n",
      "Input: 에서 화풀이 대상 같 은 데 그만둬야 할까 ?\n",
      "Predicted translation: . 이 벗어나 세요 .\n",
      "Source Sentence:  에서 화풀이 대상 같 은 데 그만둬야 할까 ?\n",
      "Model Prediction:  . 이 벗어나 세요 .\n",
      "Real:  ['점', '은', '고치', '면서', '조금', '만', '더', '버텨', '보', '세요', '.']\n",
      "Score: 0.017033\n",
      "\n",
      "Input: 하 는 사람 은 표시 가 있 었 으면 좋 겠 다\n",
      "Predicted translation: 든 사랑 한다면 다시 필요 할지 도 필요 해요 .\n",
      "Source Sentence:  하 는 사람 은 표시 가 있 었 으면 좋 겠 다\n",
      "Model Prediction:  든 사랑 한다면 다시 필요 할지 도 필요 해요 .\n",
      "Real:  ['이', '그', '신호', '를', '못', '찾', '은', '게', '아닐까요', '.']\n",
      "Score: 0.006980\n",
      "\n",
      "Input: 스타일 아니 야\n",
      "Predicted translation: 배웠 면 사과 하 면 존재 봐요 .\n",
      "Source Sentence:  스타일 아니 야\n",
      "Model Prediction:  배웠 면 사과 하 면 존재 봐요 .\n",
      "Real:  ['스타일', '도전', '해', '보', '시', '면', '어때요', '?']\n",
      "Score: 0.010182\n",
      "\n",
      "Input: 너 !\n",
      "Predicted translation: 위 드리 세요 !\n",
      "Source Sentence:  너 !\n",
      "Model Prediction:  위 드리 세요 !\n",
      "Real:  []\n",
      "Score: 0.000000\n",
      "\n",
      "Input: 날 칭찬 하 는 여자 애 . 나 한테 관심 있 는 거 아니 야 ?\n",
      "Predicted translation: 알 고 있 나 봐요 .\n",
      "Source Sentence:  날 칭찬 하 는 여자 애 . 나 한테 관심 있 는 거 아니 야 ?\n",
      "Model Prediction:  알 고 있 나 봐요 .\n",
      "Real:  ['상대방', '을', '칭찬', '할', '차례', '인', '듯', '합니다', '.']\n",
      "Score: 0.017033\n",
      "\n",
      "Input: 내 가 해야 하 지 ?\n",
      "Predicted translation: 좀 똑같 은 인간 으로 해 주 세요 .\n",
      "Source Sentence:  내 가 해야 하 지 ?\n",
      "Model Prediction:  좀 똑같 은 인간 으로 해 주 세요 .\n",
      "Real:  ['이유', '를', '찾', '는', '과정', '이', '되', '겠', '네요', '.']\n",
      "Score: 0.009134\n",
      "\n",
      "Input: 개강 이 라니\n",
      "Predicted translation: 이 참 짧 죠 .\n",
      "Source Sentence:  개강 이 라니\n",
      "Model Prediction:  이 참 짧 죠 .\n",
      "Real:  ['시작', '되', '길', '바랍니다', '.']\n",
      "Score: 0.023980\n",
      "\n",
      "Input: 우울 하 네\n",
      "Predicted translation: 한 대화 를 하 고 상담 을 받 아 보 는 게 좋 겠 어요 .\n",
      "Source Sentence:  우울 하 네\n",
      "Model Prediction:  한 대화 를 하 고 상담 을 받 아 보 는 게 좋 겠 어요 .\n",
      "Real:  ['에게', '좀', '더', '여유', '로워', '지', '세요', '.']\n",
      "Score: 0.005475\n",
      "\n",
      "Input: 나가 고 뒹굴 거 리 고 싶 어\n",
      "Predicted translation: 할 수 있 을 거 예요 .\n",
      "Source Sentence:  나가 고 뒹굴 거 리 고 싶 어\n",
      "Model Prediction:  할 수 있 을 거 예요 .\n",
      "Real:  ['수', '있', '을', '때', '하', '세요', '.']\n",
      "Score: 0.020200\n",
      "\n",
      "Input: 할수록 아프 다\n",
      "Predicted translation: 바람 라고 말 하 면 안 돼요 .\n",
      "Source Sentence:  할수록 아프 다\n",
      "Model Prediction:  바람 라고 말 하 면 안 돼요 .\n",
      "Real:  ['것', '도', '사랑', '이', '예요', '.']\n",
      "Score: 0.010802\n",
      "\n",
      "Input: 이 쓰디쓰 다\n",
      "Predicted translation: 이 좀 나아지 세요 .\n",
      "Source Sentence:  이 쓰디쓰 다\n",
      "Model Prediction:  이 좀 나아지 세요 .\n",
      "Real:  ['도', '있', '을', '거', '예요', '.']\n",
      "Score: 0.017033\n",
      "\n",
      "Input: 보 러 가 볼까 ?\n",
      "Predicted translation: 에 는 기분 전환 에 좋 겠 죠 .\n",
      "Source Sentence:  보 러 가 볼까 ?\n",
      "Model Prediction:  에 는 기분 전환 에 좋 겠 죠 .\n",
      "Real:  ['로', '보', '기', '좋', '죠', '.']\n",
      "Score: 0.014400\n",
      "\n",
      "Input: 못 차리 겠 다 .\n",
      "Predicted translation: 버리 기 어려워 요 .\n",
      "Source Sentence:  못 차리 겠 다 .\n",
      "Model Prediction:  버리 기 어려워 요 .\n",
      "Real:  ['사람', '의', '삶', '에', '한눈팔', '며', '살', '기', '에', '는', '자신', '의', '인생', '이', '너무나', '도', '소중', '합니다', '.']\n",
      "Score: 0.011304\n",
      "\n",
      "Input: 타 는 사람 한테 영통 걸까 ?\n",
      "Predicted translation: 하 다고 궁금 하 세요 .\n",
      "Source Sentence:  타 는 사람 한테 영통 걸까 ?\n",
      "Model Prediction:  하 다고 궁금 하 세요 .\n",
      "Real:  ['걸어도', '되', '는지', '물', '어', '보', '고', '걸', '어', '보', '세요', '.']\n",
      "Score: 0.016986\n",
      "\n",
      "Input: 이 넘 느려 어 플 좀 지울 까\n",
      "Predicted translation: 한가 봐요 .\n",
      "Source Sentence:  이 넘 느려 어 플 좀 지울 까\n",
      "Model Prediction:  한가 봐요 .\n",
      "Real:  ['없', '는', '어', '플', '은', '삭제', '해', '보', '세요', '.']\n",
      "Score: 0.021518\n",
      "\n",
      "Input: 땜 에 눈 아파\n",
      "Predicted translation: 일 을 적당히 관리 해 보 세요 .\n",
      "Source Sentence:  땜 에 눈 아파\n",
      "Model Prediction:  일 을 적당히 관리 해 보 세요 .\n",
      "Real:  ['마스크', '사', '드리', '고', '싶', '네요', '.']\n",
      "Score: 0.010182\n",
      "\n",
      "Input: 친구 랑 놀 까 ?\n",
      "Predicted translation: 기 쉽 지 도 몰라요 .\n",
      "Source Sentence:  친구 랑 놀 까 ?\n",
      "Model Prediction:  기 쉽 지 도 몰라요 .\n",
      "Real:  ['있', '냐고', '물', '어', '보', '세요', '.']\n",
      "Score: 0.015537\n",
      "\n",
      "Input: 잼 있 어\n",
      "Predicted translation: 을 차분 하 게 준비 있 어요 .\n",
      "Source Sentence:  잼 있 어\n",
      "Model Prediction:  을 차분 하 게 준비 있 어요 .\n",
      "Real:  ['나', '봐요', '.']\n",
      "Score: 0.010802\n",
      "\n",
      "Input: 인지 아닌지 확인 하 는 방법 있 어 ?\n",
      "Predicted translation: 번 말 하 는 게 고민 으로 힘든 거 예요 .\n",
      "Source Sentence:  인지 아닌지 확인 하 는 방법 있 어 ?\n",
      "Model Prediction:  번 말 하 는 게 고민 으로 힘든 거 예요 .\n",
      "Real:  ['저녁', '에', '약속', '을', '잡', '아', '보', '세요', '.']\n",
      "Score: 0.007576\n",
      "\n",
      "Input: 가 카톡 씹 어\n",
      "Predicted translation: 바쁜가 봐요 .\n",
      "Source Sentence:  가 카톡 씹 어\n",
      "Model Prediction:  바쁜가 봐요 .\n",
      "Real:  ['봐요', '.']\n",
      "Score: 0.027776\n",
      "\n",
      "Input: 어울릴까 ?\n",
      "Predicted translation: 보 면 좀 해 보 세요 .\n",
      "Source Sentence:  어울릴까 ?\n",
      "Model Prediction:  보 면 좀 해 보 세요 .\n",
      "Real:  ['를', '주', '는', '것', '도', '좋', '겠', '죠', '.']\n",
      "Score: 0.014284\n",
      "\n",
      "Num of Sample: 24\n",
      "Total Score: 0.014719185868064637\n"
     ]
    }
   ],
   "source": [
    "eval_bleu(enc_val[::5], dec_val[::5], verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stable-bidding",
   "metadata": {},
   "source": [
    "# 정리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "robust-journalist",
   "metadata": {},
   "source": [
    "* epoch 5 and 10\n",
    " * n_layers=2\n",
    " * vocab_size=10000\n",
    " * 문장의 문맥이 부자연스럽고 이상하다.\n",
    "\n",
    "\n",
    "* epoch 30\n",
    " * n_layers=2\n",
    " * vocab_size=10000\n",
    " * 자연스러워지고 있으나 여전히 문맥상 맞지 않는 글을 볼 수 있었다. loss는 계속 일정하게 줄어들었다.\n",
    " \n",
    " \n",
    "* epoch 30\n",
    " * n_layers=4\n",
    " * vocab_size=30000\n",
    " * n_layers와 epochs을 늘렸더니 더 자연스러워졌다. vocab_size를 늘리면서 속도가 2배이상 느려졌다.\n",
    " \n",
    " \n",
    "* 이 상태에서 vocab_size를 늘리거나 혹은 epoch을 더 늘려서 loss를 더 줄일 수 있는지 확인이 필요하다.\n",
    "* 훨씬 더 자연스러워지고 있는 질의응답이 나오고 있지만 여전히 부자연스러움은 존재한다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
